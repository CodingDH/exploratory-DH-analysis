{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Initial Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import altair as alt\n",
    "alt.renderers.enable('mimetype')\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/c_5vvt992871rr8qbqy645340000gq/T/ipykernel_65393/1901671584.py:13: DtypeWarning: Columns (4,7,57,58,59,60,61,64,65,68,69,70,94,95,96,97,98,99,117,118,119,126,127,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  firstpass_core_repos = pd.read_csv(\"../../datasets/derived_files/firstpass_core_repos.csv\")\n"
     ]
    }
   ],
   "source": [
    "# user_df = pd.read_csv(\"../../datasets/entity_files/users_dataset.csv\")\n",
    "# repo_df = pd.read_csv(\"../../datasets/large_files/entity_files/repos_dataset.csv\", low_memory=False)\n",
    "# org_df = pd.read_csv(\"../../datasets/entity_files/orgs_dataset.csv\", low_memory=False)\n",
    "initial_core_users = pd.read_csv(\"../../datasets/derived_files/initial_core_users.csv\")\n",
    "initial_core_users['origin'] = 'initial_core'\n",
    "initial_core_repos = pd.read_csv(\"../../datasets/derived_files/initial_core_repos.csv\")\n",
    "initial_core_repos['origin'] = 'initial_core'\n",
    "initial_core_orgs = pd.read_csv(\"../../datasets/derived_files/initial_core_orgs.csv\")\n",
    "initial_core_orgs['origin'] = 'initial_core'\n",
    "\n",
    "firstpass_core_users = pd.read_csv(\"../../datasets/derived_files/firstpass_core_users.csv\")\n",
    "firstpass_core_users['origin'] = 'firstpass_core'\n",
    "firstpass_core_repos = pd.read_csv(\"../../datasets/derived_files/firstpass_core_repos.csv\")\n",
    "firstpass_core_repos['origin'] = 'firstpass_core'\n",
    "firstpass_core_orgs = pd.read_csv(\"../../datasets/derived_files/firstpass_core_orgs.csv\")\n",
    "firstpass_core_orgs['origin'] = 'firstpass_core'\n",
    "\n",
    "finalpass_core_users = pd.read_csv(\"../../datasets/derived_files/finalpass_core_users.csv\")\n",
    "finalpass_core_users['origin'] = 'finalpass_core'\n",
    "finalpass_core_repos = pd.read_csv(\"../../datasets/large_files/derived_files/finalpass_core_repos.csv\", low_memory=False, on_bad_lines='skip')\n",
    "finalpass_core_repos['origin'] = 'finalpass_core'\n",
    "finalpass_core_orgs = pd.read_csv(\"../../datasets/derived_files/finalpass_core_orgs.csv\")\n",
    "finalpass_core_orgs['origin'] = 'finalpass_core'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_users = pd.concat([initial_core_users, firstpass_core_users, finalpass_core_users])\n",
    "core_repos = pd.concat([initial_core_repos, firstpass_core_repos, finalpass_core_repos])\n",
    "core_orgs = pd.concat([initial_core_orgs, firstpass_core_orgs, finalpass_core_orgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Network Connections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- user-user interactions:\n",
    "  - org_members\n",
    "  - user_orgs\n",
    "  - user_followers\n",
    "  - user_following\n",
    "\n",
    "- user-repo interactions:\n",
    "  - user_repos\n",
    "  - contirbutors_repos\n",
    "  - starrers_repos\n",
    "  - subscribers_repos\n",
    "  - forkers_repos\n",
    "  - committers_repos\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_network_connections():\n",
    "    dfs = []\n",
    "    for dir, subdir, files in os.walk(\"../../datasets/join_files/\"):\n",
    "        for f in files:\n",
    "            if ('search' not in f) and (f.endswith(\".csv\")):\n",
    "                entity_type = f.split(\"_\")[0]\n",
    "                print(f)\n",
    "                df = pd.read_csv(os.path.join(dir, f))\n",
    "                cols = df.columns\n",
    "                cols = [col for col in cols if any(x in col for x in ['full_name', 'login'])]\n",
    "                cols = [col for col in cols if ('.' not in col) or ('user.login' in col)]\n",
    "                if len(cols) > 2:\n",
    "                    cols = ['user.login', 'repo_full_name']\n",
    "                if len(cols) == 1:\n",
    "                    cols = cols + ['author.login']\n",
    "                df_dict = {}\n",
    "                df_dict['entity_type'] = entity_type\n",
    "                \n",
    "                df_dict['source'] = cols[0] if 'forks' not in f else \"owner.login\"\n",
    "                df_dict['target'] = cols[1]\n",
    "                df_dict['file_name'] = os.path.join(dir, f)\n",
    "                df_dict['file_length'] = len(df)\n",
    "                dfs.append(df_dict)\n",
    "\n",
    "    for dir, subdir, files in os.walk(\"../../datasets/large_files/join_files/\"):\n",
    "        for f in files:\n",
    "            if ('search' not in f) and (f.endswith(\".csv\")) and ('commits' not in f):\n",
    "                entity_type = f.split(\"_\")[0]\n",
    "                print(f)\n",
    "                df = pd.read_csv(os.path.join(dir, f))\n",
    "                cols = df.columns\n",
    "                cols = [col for col in cols if any(x in col for x in ['full_name', 'login'])]\n",
    "                cols = [col for col in cols if ('.' not in col) or ('user.login' in col)]\n",
    "                if len(cols) > 2:\n",
    "                    cols = ['user.login', 'repo_full_name']\n",
    "                if len(cols) == 1:\n",
    "                    cols = cols + ['author.login']\n",
    "                df_dict = {}\n",
    "                df_dict['entity_type'] = entity_type\n",
    "                df_dict['source'] = cols[0] if 'forks' not in f else \"owner.login\"\n",
    "                df_dict['target'] = cols[1]\n",
    "                df_dict['file_name'] = os.path.join(dir, f)\n",
    "                df_dict['file_length'] = len(df)\n",
    "                dfs.append(df_dict)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_mapping = {\n",
    "    '../../datasets/join_files/org_members_join_dataset.csv': 'Members that are part of orgs',\n",
    "    '../../datasets/join_files/org_followers_join_dataset.csv': 'Followers of orgs',\n",
    "    '../../datasets/join_files/repo_subscribers_join_dataset.csv': 'Subscribers of repos',\n",
    "    '../../datasets/join_files/repo_orgs_join_dataset.csv': 'Orgs of repos',\n",
    "    '../../datasets/large_files/join_files/org_repos_join_dataset.csv': 'Repos that belong to orgs',\n",
    "    '../../datasets/large_files/join_files/user_repos_join_dataset.csv': 'Repos that belong to users',\n",
    "    '../../datasets/large_files/join_files/user_following_join_dataset.csv': 'Users that are following other users',\n",
    "    '../../datasets/large_files/join_files/repo_comments_join_dataset.csv': 'Comments from repos',\n",
    "    '../../datasets/large_files/join_files/user_subscriptions_join_dataset.csv': 'Subscriptions of users',\n",
    "    '../../datasets/large_files/join_files/issues_comments_join_dataset.csv': 'Comments from repo issues',\n",
    "    '../../datasets/large_files/join_files/repo_subscribers_join_dataset.csv': 'Subscribers of repos',\n",
    "    '../../datasets/large_files/join_files/user_followers_join_dataset.csv': 'Followers of users',\n",
    "    '../../datasets/large_files/join_files/user_starred_join_dataset.csv': 'Starred repos of users',\n",
    "    '../../datasets/large_files/join_files/repo_stargazers_join_dataset.csv': 'Stargazers of repos',\n",
    "    '../../datasets/large_files/join_files/repo_contributors_join_dataset.csv': 'Contributors to repos',\n",
    "    '../../datasets/large_files/join_files/repo_issues_join_dataset.csv': 'Issues from repos',\n",
    "    '../../datasets/large_files/join_files/repo_pulls_join_dataset.csv': 'Pull requests from repos',\n",
    "    '../../datasets/large_files/join_files/user_orgs_join_dataset.csv': 'Orgs of users',\n",
    "    '../../datasets/large_files/join_files/pulls_comments_join_dataset.csv': 'Comments from repo pull requests',\n",
    "    '../../datasets/large_files/join_files/repo_forks_join_dataset.csv': 'Forks of repos',\n",
    "}\n",
    "def get_bipartite_connections(cols_df):\n",
    "    cols_df['network_type'] = None\n",
    "    for index, row in cols_df.iterrows():\n",
    "        if row.target == 'org_login':\n",
    "            network_first = \"org\"\n",
    "            network_second = 'user' if row.source == 'login' else 'repo'\n",
    "            network_type = network_first + \"_\" + network_second\n",
    "            cols_df.loc[index, 'network_type'] = network_type\n",
    "        if row.target == 'repo_full_name':\n",
    "            network_first = \"repo\"\n",
    "            if 'orgs' in row.file_name:\n",
    "                network_second = 'org'\n",
    "            else:\n",
    "                network_second = 'user' if 'login' in row.source else 'repo'\n",
    "            network_type = network_first + \"_\" + network_second\n",
    "            cols_df.loc[index, 'network_type'] = network_type\n",
    "        if row.target == 'user_login':\n",
    "            network_first = \"user\"\n",
    "            if \"orgs\" in row.file_name:\n",
    "                network_second = 'org'\n",
    "            else:\n",
    "                network_second = 'user' if 'login' in row.source else 'repo'\n",
    "            network_type = network_first + \"_\" + network_second\n",
    "            cols_df.loc[index, 'network_type'] = network_type\n",
    "        if row.source == 'repo_full_name':\n",
    "            network_type = \"repo_user\"\n",
    "            cols_df.loc[index, 'network_type'] = network_type\n",
    "\n",
    "    cols_df['descriptive_file_name'] = cols_df.file_name.map(name_mapping)\n",
    "    return cols_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptive_name(file_name):\n",
    "    # Split the file name on '/' and get the last element, then split on '_' and get the first two elements.\n",
    "    name_parts = file_name.split('/')[-1].split('_')[:2]\n",
    "    # Convert the list into a string, with the elements separated by a space.\n",
    "    descriptive_name = ' '.join(name_parts).title()\n",
    "    return descriptive_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_connections():\n",
    "    if os.path.exists(\"../../datasets/derived_files/file_totals.csv\"):\n",
    "        cols_df = pd.read_csv(\"../../datasets/derived_files/file_totals.csv\")\n",
    "    else:\n",
    "        dfs = process_network_connections()\n",
    "        cols_df = pd.DataFrame(dfs)\n",
    "        # We will initially have bipartite networks but I'm interested in projecting them to find communities as well\n",
    "        cols_df = get_bipartite_connections(cols_df)\n",
    "        # Apply the function to the 'file_name' column to create the new 'descriptive_name' column.\n",
    "        cols_df['short_file_name'] = cols_df['file_name'].apply(get_descriptive_name)\n",
    "        print(cols_df.groupby('network_type').size())\n",
    "        cols_df.to_csv(\"../../datasets/derived_files/file_totals.csv\", index=False)\n",
    "    return cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df = get_network_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/c_5vvt992871rr8qbqy645340000gq/T/ipykernel_65393/3984109063.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  cols_df.file_name = cols_df.file_name.str.replace(\"../data/\", \"../../datasets/\")\n"
     ]
    }
   ],
   "source": [
    "cols_df.file_name = cols_df.file_name.str.replace(\"../data/\", \"../../datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df['interaction_type'] = cols_df.network_type\n",
    "user_mapping = {\n",
    "    'user': 'User',\n",
    "    'repo': 'Repository',\n",
    "    'org': 'Organization',\n",
    "\n",
    "}\n",
    "def update_interaction_type(row):\n",
    "    first_item = row.interaction_type.split(\"_\")[0]\n",
    "    second_item = row.interaction_type.split(\"_\")[1]\n",
    "    first_item = user_mapping[first_item]\n",
    "    second_item = user_mapping[second_item]\n",
    "    row.interaction_type = first_item + \" - \" + second_item + \" Interaction\"\n",
    "    return row\n",
    "\n",
    "cols_df = cols_df.apply(update_interaction_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df[['descriptive_file_name', 'interaction_type', 'file_length']].to_csv(\"../../datasets/derived_files/interaction_totals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_data(cols_df):\n",
    "    org_user_dfs =[]\n",
    "    repo_user_dfs = []\n",
    "    repo_org_dfs = []\n",
    "    org_repo_dfs = []\n",
    "    user_repo_dfs = []\n",
    "    user_user_dfs = []\n",
    "    user_org_dfs = []\n",
    "\n",
    "    mapping_networks = {\n",
    "        'org_user': org_user_dfs,\n",
    "        'repo_user': repo_user_dfs,\n",
    "        'repo_org': repo_org_dfs,\n",
    "        'org_repo': org_repo_dfs,\n",
    "        'user_repo': user_repo_dfs,\n",
    "        'user_user': user_user_dfs,\n",
    "        'user_org': user_org_dfs\n",
    "    }\n",
    "\n",
    "    for _, row in cols_df.iterrows():\n",
    "        df = pd.read_csv(row['file_name'], low_memory=False)\n",
    "        target_type = row.target\n",
    "        source_type = row.source\n",
    "        print(target_type, source_type, row.file_name, row.network_type)\n",
    "        if 'org' in target_type:\n",
    "            for origin in core_orgs.origin.unique().tolist():\n",
    "                filtered_df = df[(df[row.target].isin(core_orgs[core_orgs.origin == origin].login))]\n",
    "                grouped_filtered_df = filtered_df.groupby([target_type, source_type]).size().reset_index(name='counts')\n",
    "                grouped_filtered_df['entity_type'] = row['entity_type']\n",
    "                grouped_filtered_df['file_path'] = row['file_name']\n",
    "                grouped_filtered_df['file_length'] = row['file_length']\n",
    "                grouped_filtered_df['origin'] = origin\n",
    "                grouped_filtered_df['descriptive_file_name'] = row['descriptive_file_name']\n",
    "                grouped_filtered_df['short_file_name'] = row['short_file_name']\n",
    "                grouped_filtered_df['network_type'] = row['network_type']\n",
    "                grouped_filtered_df['target_type'] = row['target']\n",
    "                grouped_filtered_df['source_type'] = row['source']\n",
    "                assign_dfs = mapping_networks[row.network_type]\n",
    "                assign_dfs.append(grouped_filtered_df)\n",
    "        if ('repo_full_name' in target_type) or ('repo_full_name' in source_type):\n",
    "            for origin in core_repos.origin.unique().tolist():\n",
    "                if row.short_file_name == \"Repo Stargazers\":\n",
    "                    filtered_df = df[(df[row.source].isin(core_repos[core_repos.origin == origin].full_name))]\n",
    "                    if (target_type == 'user.login') or (target_type == 'owner.login'):\n",
    "                        filtered_df = filtered_df.rename(columns={target_type: 'login'})\n",
    "                        updated_target_type = 'login'\n",
    "                    else:\n",
    "                        updated_target_type = source_type\n",
    "                    grouped_filtered_df = filtered_df.groupby([source_type, updated_target_type]).size().reset_index(name='counts')\n",
    "                else:\n",
    "                    filtered_df = df[(df[row.target].isin(core_repos[core_repos.origin == origin].full_name))]\n",
    "                    if (source_type == 'user.login') or (source_type == 'owner.login'):\n",
    "                        filtered_df = filtered_df.rename(columns={source_type: 'login'})\n",
    "                        updated_source_type = 'login'\n",
    "                    else:\n",
    "                        updated_source_type = source_type\n",
    "                    grouped_filtered_df = filtered_df.groupby([target_type, updated_source_type]).size().reset_index(name='counts')\n",
    "                grouped_filtered_df['entity_type'] = row['entity_type']\n",
    "                grouped_filtered_df['file_path'] = row['file_name']\n",
    "                grouped_filtered_df['file_length'] = row['file_length']\n",
    "                grouped_filtered_df['origin'] = origin\n",
    "                grouped_filtered_df['descriptive_file_name'] = row['descriptive_file_name']\n",
    "                grouped_filtered_df['short_file_name'] = row['short_file_name']\n",
    "                grouped_filtered_df['network_type'] = row['network_type']\n",
    "                grouped_filtered_df['target_type'] = row['target']\n",
    "                grouped_filtered_df['source_type'] = row['source']\n",
    "                assign_dfs = mapping_networks[row.network_type]\n",
    "                assign_dfs.append(grouped_filtered_df)\n",
    "        if 'user_login' in target_type:\n",
    "            for origin in core_users.origin.unique().tolist():\n",
    "                filtered_df = df[(df[row.target].isin(core_users[core_users.origin == origin].login))]\n",
    "                grouped_filtered_df = filtered_df.groupby([target_type, source_type]).size().reset_index(name='counts')\n",
    "                grouped_filtered_df['entity_type'] = row['entity_type']\n",
    "                grouped_filtered_df['file_path'] = row['file_name']\n",
    "                grouped_filtered_df['file_length'] = row['file_length']\n",
    "                grouped_filtered_df['origin'] = origin\n",
    "                grouped_filtered_df['descriptive_file_name'] = row['descriptive_file_name']\n",
    "                grouped_filtered_df['short_file_name'] = row['short_file_name']\n",
    "                grouped_filtered_df['network_type'] = row['network_type']\n",
    "                grouped_filtered_df['target_type'] = row['target']\n",
    "                grouped_filtered_df['source_type'] = row['source']\n",
    "                assign_dfs = mapping_networks[row.network_type]\n",
    "                assign_dfs.append(grouped_filtered_df)\n",
    "\n",
    "    return org_user_dfs, repo_user_dfs, repo_org_dfs, org_repo_dfs, user_repo_dfs, user_user_dfs, user_org_dfs      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_login login ../../datasets/join_files/org_members_join_dataset.csv org_user\n",
      "org_login login ../../datasets/join_files/org_followers_join_dataset.csv org_user\n",
      "repo_full_name login ../../datasets/join_files/repo_subscribers_join_dataset.csv repo_user\n",
      "repo_full_name login ../../datasets/join_files/repo_orgs_join_dataset.csv repo_org\n",
      "org_login full_name ../../datasets/large_files/join_files/org_repos_join_dataset.csv org_repo\n",
      "user_login full_name ../../datasets/large_files/join_files/user_repos_join_dataset.csv user_repo\n",
      "user_login login ../../datasets/large_files/join_files/user_following_join_dataset.csv user_user\n",
      "repo_full_name user.login ../../datasets/large_files/join_files/repo_comments_join_dataset.csv repo_user\n",
      "user_login full_name ../../datasets/large_files/join_files/user_subscriptions_join_dataset.csv user_repo\n",
      "repo_full_name user.login ../../datasets/large_files/join_files/issues_comments_join_dataset.csv repo_user\n",
      "repo_full_name login ../../datasets/large_files/join_files/repo_subscribers_join_dataset.csv repo_user\n",
      "user_login login ../../datasets/large_files/join_files/user_followers_join_dataset.csv user_user\n",
      "user_login full_name ../../datasets/large_files/join_files/user_starred_join_dataset.csv user_repo\n",
      "user.login repo_full_name ../../datasets/large_files/join_files/repo_stargazers_join_dataset.csv repo_user\n",
      "repo_full_name login ../../datasets/large_files/join_files/repo_contributors_join_dataset.csv repo_user\n",
      "repo_full_name user.login ../../datasets/large_files/join_files/repo_issues_join_dataset.csv repo_user\n",
      "repo_full_name user.login ../../datasets/large_files/join_files/repo_pulls_join_dataset.csv repo_user\n",
      "user_login login ../../datasets/large_files/join_files/user_orgs_join_dataset.csv user_org\n",
      "repo_full_name user.login ../../datasets/large_files/join_files/pulls_comments_join_dataset.csv repo_user\n",
      "repo_full_name owner.login ../../datasets/large_files/join_files/repo_forks_join_dataset.csv repo_user\n"
     ]
    }
   ],
   "source": [
    "org_user_dfs, repo_user_dfs, repo_org_dfs, org_repo_dfs, user_repo_dfs, user_user_dfs, user_org_dfs   = load_network_data(cols_df)\n",
    "# combined_repos_dfs = pd.concat(repos_dfs)\n",
    "# combined_orgs_dfs = pd.concat(orgs_dfs)\n",
    "# combined_users_dfs = pd.concat(users_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_login</th>\n",
       "      <th>login</th>\n",
       "      <th>counts</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_length</th>\n",
       "      <th>origin</th>\n",
       "      <th>descriptive_file_name</th>\n",
       "      <th>short_file_name</th>\n",
       "      <th>network_type</th>\n",
       "      <th>target_type</th>\n",
       "      <th>source_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC-DH</td>\n",
       "      <td>digitalkoine</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>initial_core</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHO</td>\n",
       "      <td>ColeDCrawford</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>initial_core</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHO</td>\n",
       "      <td>briancroxall</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>initial_core</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADHO</td>\n",
       "      <td>simonwiles</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>initial_core</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCDH</td>\n",
       "      <td>ttasovac</td>\n",
       "      <td>1</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>initial_core</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>sna-unipi</td>\n",
       "      <td>gatto</td>\n",
       "      <td>2</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>finalpass_core</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>sna-unipi</td>\n",
       "      <td>lyereth</td>\n",
       "      <td>2</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>finalpass_core</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>sna-unipi</td>\n",
       "      <td>shenjiaxing</td>\n",
       "      <td>2</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>finalpass_core</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>sna-unipi</td>\n",
       "      <td>zaffo1</td>\n",
       "      <td>2</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>finalpass_core</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>sna-unipi</td>\n",
       "      <td>zenith378</td>\n",
       "      <td>2</td>\n",
       "      <td>org</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>finalpass_core</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "      <td>org_user</td>\n",
       "      <td>org_login</td>\n",
       "      <td>login</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15193 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     org_login          login  counts entity_type  \\\n",
       "0       ABC-DH   digitalkoine       1         org   \n",
       "1         ADHO  ColeDCrawford       1         org   \n",
       "2         ADHO   briancroxall       1         org   \n",
       "3         ADHO     simonwiles       1         org   \n",
       "4         BCDH       ttasovac       1         org   \n",
       "..         ...            ...     ...         ...   \n",
       "469  sna-unipi          gatto       2         org   \n",
       "470  sna-unipi        lyereth       2         org   \n",
       "471  sna-unipi    shenjiaxing       2         org   \n",
       "472  sna-unipi         zaffo1       2         org   \n",
       "473  sna-unipi      zenith378       2         org   \n",
       "\n",
       "                                             file_path  file_length  \\\n",
       "0    ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "1    ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "2    ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "3    ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "4    ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "..                                                 ...          ...   \n",
       "469  ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "470  ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "471  ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "472  ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "473  ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "\n",
       "             origin          descriptive_file_name short_file_name  \\\n",
       "0      initial_core  Members that are part of orgs     Org Members   \n",
       "1      initial_core  Members that are part of orgs     Org Members   \n",
       "2      initial_core  Members that are part of orgs     Org Members   \n",
       "3      initial_core  Members that are part of orgs     Org Members   \n",
       "4      initial_core  Members that are part of orgs     Org Members   \n",
       "..              ...                            ...             ...   \n",
       "469  finalpass_core              Followers of orgs   Org Followers   \n",
       "470  finalpass_core              Followers of orgs   Org Followers   \n",
       "471  finalpass_core              Followers of orgs   Org Followers   \n",
       "472  finalpass_core              Followers of orgs   Org Followers   \n",
       "473  finalpass_core              Followers of orgs   Org Followers   \n",
       "\n",
       "    network_type target_type source_type  \n",
       "0       org_user   org_login       login  \n",
       "1       org_user   org_login       login  \n",
       "2       org_user   org_login       login  \n",
       "3       org_user   org_login       login  \n",
       "4       org_user   org_login       login  \n",
       "..           ...         ...         ...  \n",
       "469     org_user   org_login       login  \n",
       "470     org_user   org_login       login  \n",
       "471     org_user   org_login       login  \n",
       "472     org_user   org_login       login  \n",
       "473     org_user   org_login       login  \n",
       "\n",
       "[15193 rows x 12 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.concat(org_user_dfs)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'org_login': {0: 'ABC-DH'},\n",
       "  'login': {0: 'digitalkoine'},\n",
       "  'counts': {0: 2},\n",
       "  'entity_type': {0: 'org'},\n",
       "  'file_path': {0: '../../datasets/join_files/org_followers_join_dataset.csv'},\n",
       "  'file_length': {0: 5108},\n",
       "  'origin': {0: 'initial_core'},\n",
       "  'descriptive_file_name': {0: 'Followers of orgs'},\n",
       "  'short_file_name': {0: 'Org Followers'},\n",
       "  'network_type': {0: 'org_user'},\n",
       "  'target_type': {0: 'org_login'},\n",
       "  'source_type': {0: 'login'}},\n",
       " {'org_login': {0: 'ABC-DH'},\n",
       "  'login': {0: 'digitalkoine'},\n",
       "  'counts': {0: 1},\n",
       "  'entity_type': {0: 'org'},\n",
       "  'file_path': {0: '../../datasets/join_files/org_members_join_dataset.csv'},\n",
       "  'file_length': {0: 22480},\n",
       "  'origin': {0: 'initial_core'},\n",
       "  'descriptive_file_name': {0: 'Members that are part of orgs'},\n",
       "  'short_file_name': {0: 'Org Members'},\n",
       "  'network_type': {0: 'org_user'},\n",
       "  'target_type': {0: 'org_login'},\n",
       "  'source_type': {0: 'login'}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1 = test[test.short_file_name == \"Org Followers\"][0:1].to_dict()\n",
    "dict2 = test[test.short_file_name == \"Org Members\"][0:1].to_dict()\n",
    "\n",
    "combined_dicts = [dict1, dict2]\n",
    "combined_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_networks(cols_df):\n",
    "    if os.path.exists(\"../data/derived_files/repo_user_network.csv\") and os.path.exists(\"../data/derived_files/org_user_network.csv\") and os.path.exists(\"../data/derived_files/org_repo_network.csv\"):\n",
    "        grouped_combined_repos = pd.read_csv(\"../data/derived_files/repo_user_network.csv\")\n",
    "    else:\n",
    "        org_user_dfs, repo_user_dfs, repo_org_dfs, org_repo_dfs, user_repo_dfs, user_user_dfs, user_org_dfs = load_network_data(cols_df)\n",
    "        combined_org_user_dfs = pd.concat(org_user_dfs)\n",
    "        grouped_combined_org_users = combined_org_user_dfs.groupby(['org_login', 'login']).size().reset_index(name='counts')\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_type</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_length</th>\n",
       "      <th>network_type</th>\n",
       "      <th>descriptive_file_name</th>\n",
       "      <th>short_file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org</td>\n",
       "      <td>login</td>\n",
       "      <td>org_login</td>\n",
       "      <td>../../datasets/join_files/org_members_join_dat...</td>\n",
       "      <td>22480</td>\n",
       "      <td>org_user</td>\n",
       "      <td>Members that are part of orgs</td>\n",
       "      <td>Org Members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org</td>\n",
       "      <td>login</td>\n",
       "      <td>org_login</td>\n",
       "      <td>../../datasets/join_files/org_followers_join_d...</td>\n",
       "      <td>5108</td>\n",
       "      <td>org_user</td>\n",
       "      <td>Followers of orgs</td>\n",
       "      <td>Org Followers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>repo</td>\n",
       "      <td>login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/join_files/repo_subscribers_joi...</td>\n",
       "      <td>21898</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Subscribers of repos</td>\n",
       "      <td>Repo Subscribers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repo</td>\n",
       "      <td>login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/join_files/repo_orgs_join_datas...</td>\n",
       "      <td>3713</td>\n",
       "      <td>repo_org</td>\n",
       "      <td>Orgs of repos</td>\n",
       "      <td>Repo Orgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org</td>\n",
       "      <td>full_name</td>\n",
       "      <td>org_login</td>\n",
       "      <td>../../datasets/large_files/join_files/org_repo...</td>\n",
       "      <td>25542</td>\n",
       "      <td>org_repo</td>\n",
       "      <td>Repos that belong to orgs</td>\n",
       "      <td>Org Repos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user</td>\n",
       "      <td>full_name</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_rep...</td>\n",
       "      <td>1162246</td>\n",
       "      <td>user_repo</td>\n",
       "      <td>Repos that belong to users</td>\n",
       "      <td>User Repos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user</td>\n",
       "      <td>login</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_fol...</td>\n",
       "      <td>389114</td>\n",
       "      <td>user_user</td>\n",
       "      <td>Users that are following other users</td>\n",
       "      <td>User Following</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>repo</td>\n",
       "      <td>user.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_com...</td>\n",
       "      <td>1224</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Comments from repos</td>\n",
       "      <td>Repo Comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user</td>\n",
       "      <td>full_name</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_sub...</td>\n",
       "      <td>145345</td>\n",
       "      <td>user_repo</td>\n",
       "      <td>Subscriptions of users</td>\n",
       "      <td>User Subscriptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>issues</td>\n",
       "      <td>user.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/issues_c...</td>\n",
       "      <td>141566</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Comments from repo issues</td>\n",
       "      <td>Issues Comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>repo</td>\n",
       "      <td>login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_sub...</td>\n",
       "      <td>47047</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Subscribers of repos</td>\n",
       "      <td>Repo Subscribers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>user</td>\n",
       "      <td>login</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_fol...</td>\n",
       "      <td>673321</td>\n",
       "      <td>user_user</td>\n",
       "      <td>Followers of users</td>\n",
       "      <td>User Followers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>user</td>\n",
       "      <td>full_name</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_sta...</td>\n",
       "      <td>1410741</td>\n",
       "      <td>user_repo</td>\n",
       "      <td>Starred repos of users</td>\n",
       "      <td>User Starred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>repo</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>user.login</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_sta...</td>\n",
       "      <td>33097</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Stargazers of repos</td>\n",
       "      <td>Repo Stargazers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>repo</td>\n",
       "      <td>login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_con...</td>\n",
       "      <td>120151</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Contributors to repos</td>\n",
       "      <td>Repo Contributors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>repo</td>\n",
       "      <td>user.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_iss...</td>\n",
       "      <td>90552</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Issues from repos</td>\n",
       "      <td>Repo Issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>repo</td>\n",
       "      <td>user.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_pul...</td>\n",
       "      <td>30908</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Pull requests from repos</td>\n",
       "      <td>Repo Pulls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>user</td>\n",
       "      <td>login</td>\n",
       "      <td>user_login</td>\n",
       "      <td>../../datasets/large_files/join_files/user_org...</td>\n",
       "      <td>5713</td>\n",
       "      <td>user_org</td>\n",
       "      <td>Orgs of users</td>\n",
       "      <td>User Orgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pulls</td>\n",
       "      <td>user.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/pulls_co...</td>\n",
       "      <td>13262</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Comments from repo pull requests</td>\n",
       "      <td>Pulls Comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>repo</td>\n",
       "      <td>owner.login</td>\n",
       "      <td>repo_full_name</td>\n",
       "      <td>../../datasets/large_files/join_files/repo_for...</td>\n",
       "      <td>32329</td>\n",
       "      <td>repo_user</td>\n",
       "      <td>Forks of repos</td>\n",
       "      <td>Repo Forks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity_type          source          target  \\\n",
       "0          org           login       org_login   \n",
       "1          org           login       org_login   \n",
       "2         repo           login  repo_full_name   \n",
       "3         repo           login  repo_full_name   \n",
       "4          org       full_name       org_login   \n",
       "5         user       full_name      user_login   \n",
       "6         user           login      user_login   \n",
       "7         repo      user.login  repo_full_name   \n",
       "8         user       full_name      user_login   \n",
       "9       issues      user.login  repo_full_name   \n",
       "10        repo           login  repo_full_name   \n",
       "11        user           login      user_login   \n",
       "12        user       full_name      user_login   \n",
       "13        repo  repo_full_name      user.login   \n",
       "14        repo           login  repo_full_name   \n",
       "15        repo      user.login  repo_full_name   \n",
       "16        repo      user.login  repo_full_name   \n",
       "17        user           login      user_login   \n",
       "18       pulls      user.login  repo_full_name   \n",
       "19        repo     owner.login  repo_full_name   \n",
       "\n",
       "                                            file_name  file_length  \\\n",
       "0   ../../datasets/join_files/org_members_join_dat...        22480   \n",
       "1   ../../datasets/join_files/org_followers_join_d...         5108   \n",
       "2   ../../datasets/join_files/repo_subscribers_joi...        21898   \n",
       "3   ../../datasets/join_files/repo_orgs_join_datas...         3713   \n",
       "4   ../../datasets/large_files/join_files/org_repo...        25542   \n",
       "5   ../../datasets/large_files/join_files/user_rep...      1162246   \n",
       "6   ../../datasets/large_files/join_files/user_fol...       389114   \n",
       "7   ../../datasets/large_files/join_files/repo_com...         1224   \n",
       "8   ../../datasets/large_files/join_files/user_sub...       145345   \n",
       "9   ../../datasets/large_files/join_files/issues_c...       141566   \n",
       "10  ../../datasets/large_files/join_files/repo_sub...        47047   \n",
       "11  ../../datasets/large_files/join_files/user_fol...       673321   \n",
       "12  ../../datasets/large_files/join_files/user_sta...      1410741   \n",
       "13  ../../datasets/large_files/join_files/repo_sta...        33097   \n",
       "14  ../../datasets/large_files/join_files/repo_con...       120151   \n",
       "15  ../../datasets/large_files/join_files/repo_iss...        90552   \n",
       "16  ../../datasets/large_files/join_files/repo_pul...        30908   \n",
       "17  ../../datasets/large_files/join_files/user_org...         5713   \n",
       "18  ../../datasets/large_files/join_files/pulls_co...        13262   \n",
       "19  ../../datasets/large_files/join_files/repo_for...        32329   \n",
       "\n",
       "   network_type                 descriptive_file_name     short_file_name  \n",
       "0      org_user         Members that are part of orgs         Org Members  \n",
       "1      org_user                     Followers of orgs       Org Followers  \n",
       "2     repo_user                  Subscribers of repos    Repo Subscribers  \n",
       "3      repo_org                         Orgs of repos           Repo Orgs  \n",
       "4      org_repo             Repos that belong to orgs           Org Repos  \n",
       "5     user_repo            Repos that belong to users          User Repos  \n",
       "6     user_user  Users that are following other users      User Following  \n",
       "7     repo_user                   Comments from repos       Repo Comments  \n",
       "8     user_repo                Subscriptions of users  User Subscriptions  \n",
       "9     repo_user             Comments from repo issues     Issues Comments  \n",
       "10    repo_user                  Subscribers of repos    Repo Subscribers  \n",
       "11    user_user                    Followers of users      User Followers  \n",
       "12    user_repo                Starred repos of users        User Starred  \n",
       "13    repo_user                   Stargazers of repos     Repo Stargazers  \n",
       "14    repo_user                 Contributors to repos   Repo Contributors  \n",
       "15    repo_user                     Issues from repos         Repo Issues  \n",
       "16    repo_user              Pull requests from repos          Repo Pulls  \n",
       "17     user_org                         Orgs of users           User Orgs  \n",
       "18    repo_user      Comments from repo pull requests      Pulls Comments  \n",
       "19    repo_user                        Forks of repos          Repo Forks  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo_full_name login ../data/join_files/repo_subscribers_join_dataset.csv\n",
      "repo_full_name user.login ../data/large_files/join_files/repo_comments_join_dataset.csv\n",
      "repo_full_name user.login ../data/large_files/join_files/issues_comments_join_dataset.csv\n",
      "repo_full_name login ../data/large_files/join_files/repo_subscribers_join_dataset.csv\n",
      "user.login repo_full_name ../data/large_files/join_files/repo_stargazers_join_dataset.csv\n",
      "repo_full_name login ../data/large_files/join_files/repo_contributors_join_dataset.csv\n",
      "repo_full_name user.login ../data/large_files/join_files/repo_issues_join_dataset.csv\n",
      "repo_full_name user.login ../data/large_files/join_files/repo_pulls_join_dataset.csv\n",
      "repo_full_name user.login ../data/large_files/join_files/pulls_comments_join_dataset.csv\n",
      "repo_full_name owner.login ../data/large_files/join_files/repo_forks_join_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "def build_networks(cols_df):\n",
    "    networks_paths = {\n",
    "        'org_user': \"../../datasets/derived_files/org_user_network.csv\",\n",
    "        'repo_user': \"../../datasets/derived_files/repo_user_network.csv\",\n",
    "        'repo_org': \"../../datasets/derived_files/repo_org_network.csv\",\n",
    "        'org_repo': \"../../datasets/derived_files/org_repo_network.csv\",\n",
    "        'user_repo': \"../../datasets/derived_files/user_repo_network.csv\",\n",
    "        'user_user': \"../../datasets/derived_files/user_user_network.csv\",\n",
    "        'user_org': \"../../datasets/derived_files/user_org_network.csv\"\n",
    "    }\n",
    "\n",
    "    for network, file_path in networks_paths.items():\n",
    "        if os.path.exists(file_path):\n",
    "            yield pd.read_csv(file_path)\n",
    "        else:\n",
    "            org_user_dfs, repo_user_dfs, repo_org_dfs, org_repo_dfs, user_repo_dfs, user_user_dfs, user_org_dfs = load_network_data(cols_df)\n",
    "            dfs_mapping = {\n",
    "                'org_user': org_user_dfs,\n",
    "                'repo_user': repo_user_dfs,\n",
    "                'repo_org': repo_org_dfs,\n",
    "                'org_repo': org_repo_dfs,\n",
    "                'user_repo': user_repo_dfs,\n",
    "                'user_user': user_user_dfs,\n",
    "                'user_org': user_org_dfs\n",
    "            }\n",
    "            df = dfs_mapping[network]\n",
    "            network_row = cols_df[cols_df.network_type == network]\n",
    "            combined_df = pd.concat(df)\n",
    "            grouped_df = combined_df.groupby(['org_login', 'login']).size().reset_index(name='counts')\n",
    "            grouped_df.to_csv(file_path, index=False)\n",
    "            yield grouped_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_repos = combined_repos_dfs.groupby(['repo_full_name', 'login'])['counts'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_repos.to_csv(\"../data/derived_files/repo_user_network.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_repos = pd.read_csv(\"../data/derived_files/repo_user_network.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pivoted_df = pd.pivot(grouped_combined_repos, index='repo_full_name', columns='login', values='counts').reset_index()\n",
    "pivoted_df = pivoted_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = pivoted_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_df.to_csv(\"../data/derived_files/repo_user_network_pivoted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.sparse import csr_matrix\n",
    "\n",
    "# # Set up the UMAP parameters\n",
    "# umap_params = {\n",
    "#     \"n_neighbors\": 30,\n",
    "#     \"min_dist\": 0.3,\n",
    "#     \"n_components\": 2,\n",
    "#     \"metric\": 'euclidean',\n",
    "#     \"random_state\": 42\n",
    "# }\n",
    "\n",
    "# # Assume df_umap is your dataframe\n",
    "# sparse_matrix = csr_matrix(pivoted_df.values)\n",
    "\n",
    "# # Initialize UMAP\n",
    "# reducer = umap.UMAP(**umap_params)\n",
    "\n",
    "# # Fit the model and transform the data\n",
    "# embedding = reducer.fit_transform(sparse_matrix)\n",
    "\n",
    "# # Plot the UMAP projection\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1)\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title('UMAP projection of repo_full_name and login interactions', fontsize=12)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zleblanc/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/zleblanc/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/zleblanc/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/zleblanc/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import umap\n",
    "# import hdbscan\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "# Set up the UMAP parameters\n",
    "umap_params = {\n",
    "    \"n_neighbors\": 20,\n",
    "    \"min_dist\": 0.5,\n",
    "    # \"n_components\": 2,\n",
    "    \"metric\": 'euclidean',\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "df_umap = pivoted_df.copy()\n",
    "# Initialize UMAP\n",
    "reducer = umap.UMAP(**umap_params)\n",
    "\n",
    "# Fit the model and transform the data\n",
    "embedding = reducer.fit_transform(df_umap.values)\n",
    "\n",
    "# Create a HDBSCAN clusterer\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=15, gen_min_span_tree=True)\n",
    "\n",
    "# # Fit the clusterer to the UMAP embeddings\n",
    "# clusterer.fit(embedding)\n",
    "\n",
    "# # Generate the cluster labels\n",
    "# color_palette = sns.color_palette('deep', max(clusterer.labels_) + 1)\n",
    "# cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "# cluster_member_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n",
    "\n",
    "# # Plot the UMAP projection with cluster colors\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1, c=cluster_member_colors)\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title('UMAP projection of repo_full_name and login interactions, colored by cluster', fontsize=12)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the embedding vectors in a TSV file\n",
    "np.savetxt(os.path.join('../outputs', 'vecs.tsv'), embedding, delimiter='\\t')\n",
    "\n",
    "# Save the labels in a metadata file\n",
    "with open(os.path.join('../outputs', 'metadata.tsv'), 'w') as f:\n",
    "    for label in df_umap.index:\n",
    "        f.write('{}\\n'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv(\"../outputs/vecs.tsv\", sep=\"\\t\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../outputs/log_dir/embedding.ckpt-1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "import os\n",
    "# assuming `vecs` is a 2D array or matrix containing your embeddings and `metadata` is a list of labels\n",
    "\n",
    "# save the weights you're probing as a variable\n",
    "emb = tf.Variable(embedding, name='embedding')\n",
    "log_dir = \"../outputs/log_dir\"\n",
    "checkpoint = tf.train.Checkpoint(embedding=emb)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "# checkpoint \n",
    "# sess = tf.Session()\n",
    "# saver = tf.train.Saver()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver.save(sess, os.path.join(log_dir, 'model.ckpt'))\n",
    "\n",
    "# config\n",
    "# config = projector.ProjectorConfig()\n",
    "# embedding = config.embeddings.add()\n",
    "# embedding.tensor_name = 'embedding:0'  # Name of the tensor in the TensorFlow graph\n",
    "# embedding.metadata_path = os.path.join(log_dir, 'metadata.tsv')  # Path to your labels\n",
    "\n",
    "# # write labels\n",
    "# # with open(embedding.metadata_path, 'w') as metadata_file:\n",
    "# #     for row in df_umap.index:\n",
    "# #         metadata_file.write('{}\\n'.format(row))\n",
    "\n",
    "# # saves a config file that TensorBoard will read during startup.\n",
    "# projector.visualize_embeddings(log_dir, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = grouped_combined_repos.repo_full_name.unique().tolist() + grouped_combined_repos.login.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = 'embedding:0'  # Name of the tensor in the TensorFlow graph\n",
    "embedding.metadata_path = os.path.join(log_dir, 'metadata.tsv')  # Path to your labels\n",
    "\n",
    "# write labels\n",
    "with open(embedding.metadata_path, 'w') as metadata_file:\n",
    "    for row in metadata_list:\n",
    "        metadata_file.write('{}\\n'.format(row))\n",
    "\n",
    "# saves a config file that TensorBoard will read during startup.\n",
    "projector.visualize_embeddings(log_dir, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir \"../outputs/log_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login\n",
      "0--key          0\n",
      "0-Eclipse-0     0\n",
      "0-kaladin       0\n",
      "0-wiz-0         0\n",
      "00-Evan         0\n",
      "               ..\n",
      "zzuduoduo       0\n",
      "zzuieliyaoli    0\n",
      "zzy14           0\n",
      "zzzyy           0\n",
      "zzzzBov         0\n",
      "Length: 90501, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_umap.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14769, 90501)\n"
     ]
    }
   ],
   "source": [
    "print(df_umap.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_orgs_dfs = pd.concat(orgs_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'org_login': {0: 'ABC-DH'},\n",
       " 'login': {0: 'digitalkoine'},\n",
       " 'counts': {0: 1},\n",
       " 'entity_type': {0: 'org'},\n",
       " 'file_path': {0: '../data/join_files/org_members_join_dataset.csv'},\n",
       " 'join_type': {0: 'org_members'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_orgs_dfs[0:1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18823, 408878)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_grouped_combined_repos = grouped_combined_repos[grouped_combined_repos.counts > 2]\n",
    "len(subset_grouped_combined_repos), subset_grouped_combined_repos.counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to graph from 'org_login' and 'login' columns\n",
    "repo_nodes = list(subset_grouped_combined_repos['repo_full_name'].unique())\n",
    "login_nodes = list(subset_grouped_combined_repos['login'].unique())\n",
    "\n",
    "bipartite_graph.add_nodes_from(repo_nodes, bipartite=0)  # Add the nodes to the 'org_login' partition\n",
    "bipartite_graph.add_nodes_from(login_nodes, bipartite=1)  # Add the nodes to the 'login' partition\n",
    "\n",
    "# Prepare edges subset_grouped_combined_repos for adding to the graph\n",
    "edges = []\n",
    "for _, row in subset_grouped_combined_repos.iterrows():\n",
    "    edge = (row['repo_full_name'], row['login'], {\n",
    "        'weight': row['counts'],\n",
    "    })\n",
    "    edges.append(edge)\n",
    "\n",
    "# Add edges to graph\n",
    "bipartite_graph.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# G = bipartite.projected_graph(bipartite_graph, repo_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_graph = bipartite.weighted_projected_graph(bipartite_graph, login_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [G for G in nx.connected_components(user_graph)]\n",
    "largest = max(components, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_graph = user_graph.subgraph(largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83311"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_graph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m partition \u001b[39m=\u001b[39m community_louvain\u001b[39m.\u001b[39mbest_partition(user_graph)\n\u001b[1;32m     10\u001b[0m \u001b[39m# draw the graph\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pos \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39;49mspring_layout(user_graph)\n\u001b[1;32m     13\u001b[0m \u001b[39m# color the nodes according to their partition\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cmap \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mcm\u001b[39m.\u001b[39mget_cmap(\u001b[39m'\u001b[39m\u001b[39mviridis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mmax\u001b[39m(partition\u001b[39m.\u001b[39mvalues()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 21:4\u001b[0m, in \u001b[0;36margmap_spring_layout_18\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/networkx/drawing/layout.py:481\u001b[0m, in \u001b[0;36mspring_layout\u001b[0;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[1;32m    479\u001b[0m         nnodes, _ \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mshape\n\u001b[1;32m    480\u001b[0m         k \u001b[39m=\u001b[39m dom_size \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(nnodes)\n\u001b[0;32m--> 481\u001b[0m     pos \u001b[39m=\u001b[39m _sparse_fruchterman_reingold(\n\u001b[1;32m    482\u001b[0m         A, k, pos_arr, fixed, iterations, threshold, dim, seed\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     A \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mto_numpy_array(G, weight\u001b[39m=\u001b[39mweight)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 25:4\u001b[0m, in \u001b[0;36margmap__sparse_fruchterman_reingold_22\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/networkx/drawing/layout.py:623\u001b[0m, in \u001b[0;36m_sparse_fruchterman_reingold\u001b[0;34m(A, k, pos, fixed, iterations, threshold, dim, seed)\u001b[0m\n\u001b[1;32m    621\u001b[0m     Ai \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mgetrowview(i)\u001b[39m.\u001b[39mtoarray()  \u001b[39m# TODO: revisit w/ sparse 1D container\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[39m# displacement \"force\"\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m     displacement[:, i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    624\u001b[0m         delta \u001b[39m*\u001b[39;49m (k \u001b[39m*\u001b[39;49m k \u001b[39m/\u001b[39;49m distance\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m \u001b[39m-\u001b[39;49m Ai \u001b[39m*\u001b[39;49m distance \u001b[39m/\u001b[39;49m k)\n\u001b[1;32m    625\u001b[0m     )\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    626\u001b[0m \u001b[39m# update positions\u001b[39;00m\n\u001b[1;32m    627\u001b[0m length \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt((displacement\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "\n",
    "# assuming G is your graph\n",
    "\n",
    "# compute the best partition using Louvain method\n",
    "partition = community_louvain.best_partition(user_graph)\n",
    "\n",
    "# draw the graph\n",
    "pos = nx.spring_layout(user_graph)\n",
    "\n",
    "# color the nodes according to their partition\n",
    "cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(user_graph, pos, partition.keys(), node_size=10, \n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(user_graph, pos, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(user_graph, \"../outputs/user_graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = nx.to_pandas_adjacency(user_graph, nodelist=login_nodes, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot assign slice from input of different size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m reducer \u001b[39m=\u001b[39m umap\u001b[39m.\u001b[39mUMAP(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mumap_params)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Fit the model and transform the data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m embedding \u001b[39m=\u001b[39m reducer\u001b[39m.\u001b[39;49mfit_transform(df_umap\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Plot the UMAP projection\u001b[39;00m\n\u001b[1;32m     21\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/umap_.py:2772\u001b[0m, in \u001b[0;36mUMAP.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2743\u001b[0m     \u001b[39m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[1;32m   2744\u001b[0m \u001b[39m    output.\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[39m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2772\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m   2773\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2774\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dens:\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/umap_.py:2516\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2510\u001b[0m     nn_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_distance_func\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mknn_dists \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2512\u001b[0m     (\n\u001b[1;32m   2513\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_indices,\n\u001b[1;32m   2514\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_dists,\n\u001b[1;32m   2515\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_search_index,\n\u001b[0;32m-> 2516\u001b[0m     ) \u001b[39m=\u001b[39m nearest_neighbors(\n\u001b[1;32m   2517\u001b[0m         X[index],\n\u001b[1;32m   2518\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_neighbors,\n\u001b[1;32m   2519\u001b[0m         nn_metric,\n\u001b[1;32m   2520\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metric_kwds,\n\u001b[1;32m   2521\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mangular_rp_forest,\n\u001b[1;32m   2522\u001b[0m         random_state,\n\u001b[1;32m   2523\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlow_memory,\n\u001b[1;32m   2524\u001b[0m         use_pynndescent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2525\u001b[0m         n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m   2526\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   2527\u001b[0m     )\n\u001b[1;32m   2528\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_knn_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mknn_indices\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/umap/umap_.py:328\u001b[0m, in \u001b[0;36mnearest_neighbors\u001b[0;34m(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    325\u001b[0m     n_trees \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m64\u001b[39m, \u001b[39m5\u001b[39m \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m/\u001b[39m \u001b[39m20.0\u001b[39m)))\n\u001b[1;32m    326\u001b[0m     n_iters \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(np\u001b[39m.\u001b[39mlog2(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))))\n\u001b[0;32m--> 328\u001b[0m     knn_search_index \u001b[39m=\u001b[39m NNDescent(\n\u001b[1;32m    329\u001b[0m         X,\n\u001b[1;32m    330\u001b[0m         n_neighbors\u001b[39m=\u001b[39;49mn_neighbors,\n\u001b[1;32m    331\u001b[0m         metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    332\u001b[0m         metric_kwds\u001b[39m=\u001b[39;49mmetric_kwds,\n\u001b[1;32m    333\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    334\u001b[0m         n_trees\u001b[39m=\u001b[39;49mn_trees,\n\u001b[1;32m    335\u001b[0m         n_iters\u001b[39m=\u001b[39;49mn_iters,\n\u001b[1;32m    336\u001b[0m         max_candidates\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[1;32m    337\u001b[0m         low_memory\u001b[39m=\u001b[39;49mlow_memory,\n\u001b[1;32m    338\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    339\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    340\u001b[0m         compressed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m     knn_indices, knn_dists \u001b[39m=\u001b[39m knn_search_index\u001b[39m.\u001b[39mneighbor_graph\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pynndescent/pynndescent_.py:804\u001b[0m, in \u001b[0;36mNNDescent.__init__\u001b[0;34m(self, data, metric, metric_kwds, n_neighbors, n_trees, leaf_size, pruning_degree_multiplier, diversify_prob, n_search_trees, tree_init, init_graph, init_dist, random_state, low_memory, max_candidates, n_iters, delta, n_jobs, compressed, parallel_batch_queries, verbose)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[39mprint\u001b[39m(ts(), \u001b[39m\"\u001b[39m\u001b[39mBuilding RP forest with\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(n_trees), \u001b[39m\"\u001b[39m\u001b[39mtrees\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    794\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rp_forest \u001b[39m=\u001b[39m make_forest(\n\u001b[1;32m    795\u001b[0m         data,\n\u001b[1;32m    796\u001b[0m         n_neighbors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_angular_trees,\n\u001b[1;32m    803\u001b[0m     )\n\u001b[0;32m--> 804\u001b[0m     leaf_array \u001b[39m=\u001b[39m rptree_leaf_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rp_forest)\n\u001b[1;32m    805\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rp_forest \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pynndescent/rp_trees.py:1097\u001b[0m, in \u001b[0;36mrptree_leaf_array\u001b[0;34m(rp_forest)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrptree_leaf_array\u001b[39m(rp_forest):\n\u001b[1;32m   1096\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rp_forest) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1097\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvstack(rptree_leaf_array_parallel(rp_forest))\n\u001b[1;32m   1098\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/pynndescent/rp_trees.py:1089\u001b[0m, in \u001b[0;36mrptree_leaf_array_parallel\u001b[0;34m(rp_forest)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrptree_leaf_array_parallel\u001b[39m(rp_forest):\n\u001b[0;32m-> 1089\u001b[0m     result \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mParallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\n\u001b[1;32m   1090\u001b[0m         joblib\u001b[39m.\u001b[39;49mdelayed(get_leaves_from_tree)(rp_tree) \u001b[39mfor\u001b[39;49;00m rp_tree \u001b[39min\u001b[39;49;00m rp_forest\n\u001b[1;32m   1091\u001b[0m     )\n\u001b[1;32m   1092\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:620\u001b[0m, in \u001b[0;36mSafeFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    621\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    622\u001b[0m         \u001b[39m# We capture the KeyboardInterrupt and reraise it as\u001b[39;00m\n\u001b[1;32m    623\u001b[0m         \u001b[39m# something different, as multiprocessing does not\u001b[39;00m\n\u001b[1;32m    624\u001b[0m         \u001b[39m# interrupt processing for a KeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    625\u001b[0m         \u001b[39mraise\u001b[39;00m WorkerInterrupt() \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.virtualenvs/values_and_versions_env/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot assign slice from input of different size"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the UMAP parameters\n",
    "umap_params = {\n",
    "    \"n_neighbors\": 30,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": 'euclidean',\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "df_umap = user_matrix.copy()\n",
    "# Initialize UMAP\n",
    "reducer = umap.UMAP(**umap_params)\n",
    "\n",
    "# Fit the model and transform the data\n",
    "embedding = reducer.fit_transform(df_umap.values)\n",
    "\n",
    "# Plot the UMAP projection\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of repo_full_name and login interactions', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_nodes = core_users['login'].tolist()\n",
    "repo_nodes = core_repos['full_name'].tolist()\n",
    "org_nodes = core_orgs['login'].tolist()\n",
    "user_nodes = user_nodes + org_nodes\n",
    "\n",
    "edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 2264, 4983)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_nodes), len(repo_nodes), len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_graph.add_nodes_from(user_nodes, bipartite=0)\n",
    "bipartite_graph.add_nodes_from(repo_nodes, bipartite=1)\n",
    "bipartite_graph.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected? False\n",
      "bipartite? True\n"
     ]
    }
   ],
   "source": [
    "print('connected?', nx.is_connected(bipartite_graph))\n",
    "print('bipartite?', nx.is_bipartite(bipartite_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(bipartite_graph,\"../data/derived_files/bipartite_graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel(\"../data/private_data/Appendix A publication_raw_chun_llc_dhq.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = pd.read_excel(\"../data/private_data/Appendix E co-retweet network.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "values_and_versions_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
