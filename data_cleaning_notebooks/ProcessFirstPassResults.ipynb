{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process First Pass Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_add_orgs, check_add_repos, check_add_users, check_for_joins_in_older_queries, check_return_error_file, check_for_entity_in_older_queries, check_if_older_file_exists\n",
    "from data_generation_scripts.generate_user_repos_interactions import get_user_repo_activities\n",
    "from data_generation_scripts.generate_translations import check_detect_language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have run the `ProcessDHRepos`, `ProcessDHUsers`, and `ProcessDHOrgs` notebooks, we can start to expand our dataset. Our initial first pass expansion will be to add the following information to our dataset:\n",
    "\n",
    "- owners of `initial_core_repos` will be added to `core_users` and `core_orgs`\n",
    "- repos of `core_users` and `core_orgs` will be added to `core_repos`\n",
    "\n",
    "We may do some thresholding to avoid too many users or repos being added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"../data/entity_files/users_dataset.csv\")\n",
    "repo_df = pd.read_csv(\"../data/large_files/entity_files/repos_dataset.csv\", low_memory=False)\n",
    "org_df = pd.read_csv(\"../data/entity_files/orgs_dataset.csv\", low_memory=False)\n",
    "search_queries_repo_join_df = pd.read_csv(\"../data/derived_files/updated_search_queries_repo_join_subset_dh_dataset.csv\")\n",
    "search_queries_user_join_df = pd.read_csv(\n",
    "    \"../data/derived_files/updated_search_queries_user_join_subset_dh_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_users = pd.read_csv(\"../data/derived_files/initial_core_users.csv\")\n",
    "initial_core_repos = pd.read_csv(\"../data/derived_files/initial_core_repos.csv\")\n",
    "initial_core_orgs = pd.read_csv(\"../data/derived_files/initial_core_orgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_members_df = pd.read_csv(\"../data/join_files/org_members_join_dataset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Potential New Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_users = search_queries_repo_join_df[~search_queries_repo_join_df['owner.login'].isin(user_df.login)]\n",
    "missing_users = missing_users[missing_users['owner.type'] == 'User']\n",
    "missing_orgs = search_queries_repo_join_df[~search_queries_repo_join_df['owner.login'].isin(org_df.login)]\n",
    "missing_orgs = missing_orgs[missing_orgs['owner.type'] == 'Organization']\n",
    "len(missing_users), len(missing_orgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_users = missing_users[['owner.login']]\n",
    "missing_users = missing_users.rename(columns={'owner.login': 'login'})\n",
    "missing_users['type'] = 'User'\n",
    "missing_users['url'] = missing_users.login.apply(lambda x: f\"https://api.github.com/users/{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_org_members = org_members_df[(org_members_df['org_login'].isin(initial_core_orgs.login)) ]\n",
    "initial_org_members = initial_org_members[~initial_org_members['login'].isin(initial_core_users.login)]\n",
    "additional_missing_users = initial_org_members[~initial_org_members['login'].isin(user_df.login)]\n",
    "additional_missing_users = additional_missing_users[['login', 'url', 'type']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_users = pd.concat([missing_users, additional_missing_users])\n",
    "len(missing_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_orgs = missing_orgs[['owner.login']]\n",
    "missing_orgs = missing_orgs.rename(columns={'owner.login': 'login'})\n",
    "missing_orgs['type'] = 'Organization'\n",
    "missing_orgs['url'] = missing_orgs.login.apply(lambda x: f\"https://api.github.com/orgs/{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Orgs: 100%|██████████| 3/3 [00:10<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file_name  file_size  \\\n",
      "28  repos_dataset_2022_11_04.csv   39255456   \n",
      "14  users_dataset_2023_06_02.csv   25265947   \n",
      "\n",
      "                            directory       date  \n",
      "28  ../data/older_files/entity_files/ 2022-11-04  \n",
      "14  ../data/older_files/entity_files/ 2023-06-02  \n",
      "Number of new users: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users:   0%|          | 0/3 [00:47<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file_name  file_size  \\\n",
      "29  repos_dataset_2022_11_04.csv   39255456   \n",
      "15  users_dataset_2023_06_03.csv   33404563   \n",
      "\n",
      "                            directory       date  \n",
      "29  ../data/older_files/entity_files/ 2022-11-04  \n",
      "15  ../data/older_files/entity_files/ 2023-06-03  \n"
     ]
    }
   ],
   "source": [
    "if len(missing_orgs) > 0:\n",
    "    org_df = check_add_orgs(missing_orgs, '../data/entity_files/orgs_dataset.csv', True, False)\n",
    "if len(missing_users) > 0:\n",
    "    user_df = check_add_users(missing_users, '../data/entity_files/users_dataset.csv', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_org_members = user_df[user_df.login.isin(initial_org_members.login)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 1783, 190, 736)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_core_users = user_df[(user_df.login.isin(search_queries_repo_join_df['owner.login']))]\n",
    "expanded_core_users = expanded_core_users[~expanded_core_users.login.isin(initial_core_users.login)]\n",
    "expanded_core_orgs = expanded_core_users[expanded_core_users['type'] == 'Organization']\n",
    "expanded_core_users = expanded_core_users[expanded_core_users['type'] == 'User']\n",
    "expanded_core_orgs = expanded_core_orgs[~expanded_core_orgs.login.isin(initial_core_orgs.login)]\n",
    "expanded_core_users = pd.concat([expanded_core_users, expanded_org_members])\n",
    "len(expanded_core_orgs), len(expanded_core_users), len(initial_core_orgs), len(initial_core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8878.0\n",
      "3134.0\n",
      "1932.0 594.0\n"
     ]
    }
   ],
   "source": [
    "filter_columns = ['public_repos',\n",
    "#  'public_gists',\n",
    "#  'followers',\n",
    "#  'following',\n",
    "#  'star_count'\n",
    " ]\n",
    "filtered_initial_core_users = initial_core_users.copy()\n",
    "filtered_initial_core_orgs = initial_core_orgs.copy()\n",
    "print(filtered_initial_core_users.public_repos.sum())\n",
    "print(filtered_initial_core_orgs.public_repos.sum())\n",
    "for col in filter_columns:\n",
    "    filtered_initial_core_users = filtered_initial_core_users[(filtered_initial_core_users[col].ge(filtered_initial_core_users[col].quantile(q=.25))) & (filtered_initial_core_users[col].le(filtered_initial_core_users[col].quantile(q=.75)))]\n",
    "    filtered_initial_core_orgs = filtered_initial_core_orgs[(filtered_initial_core_orgs[col].ge(filtered_initial_core_orgs[col].quantile(q=.25))) & (filtered_initial_core_orgs[col].le(filtered_initial_core_orgs[col].quantile(q=.75)))]\n",
    "print(filtered_initial_core_users.public_repos.sum(), filtered_initial_core_orgs.public_repos.sum())\n",
    "filtered_users = pd.concat([filtered_initial_core_users, filtered_initial_core_orgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 1783)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_users), len(expanded_core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_users = pd.concat([initial_core_users, initial_core_orgs])\n",
    "len(filtered_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_repos = 0\n",
    "for _, row in filtered_users.iterrows():\n",
    "    existing_repos = repo_df[repo_df['owner.login'] == row['login']]\n",
    "    if len(existing_repos) != row['public_repos']:\n",
    "        # print(f\"For user {row['login']} there are {len(existing_repos)} repos but they have {row['public_repos']} public repos\")\n",
    "        total_missing_repos += row['public_repos'] - len(existing_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-773.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_missing_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_missing_repos > 0:\n",
    "    from datetime import datetime\n",
    "    user_repos_output_path = \"../data/large_files/join_files/user_repos_join_dataset.csv\"\n",
    "    repos_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "    users_output_path = \"../data/entity_files/users_dataset.csv\"\n",
    "    get_url_field = \"repos_url\"\n",
    "    load_existing_files = False\n",
    "    overwrite_existing_temp_files = False\n",
    "    join_unique_field = 'user_login'\n",
    "    filter_fields = ['user_login', 'full_name']\n",
    "    retry_errors = True\n",
    "\n",
    "    user_repos_df, user_df = get_user_repo_activities(filtered_users,user_repos_output_path, users_output_path, get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields, retry_errors)\n",
    "\n",
    "    grouped_user_repos_df = user_repos_df['user_login'].value_counts().reset_index().rename(columns={'index': 'login', 'user_login': f'new_public_repos'})\n",
    "    merged_df = pd.merge(filtered_users[['login', 'public_repos']], grouped_user_repos_df, on='login', how='left')\n",
    "    merged_df.new_public_repos.fillna(0, inplace=True)\n",
    "    new_repos = user_repos_df[~user_repos_df.full_name.isin(repo_df.full_name)]\n",
    "    repo_headers = pd.read_csv('../data/metadata_files/repo_headers.csv')\n",
    "    new_repo_df = new_repos[repo_headers.columns]\n",
    "    repo_df = pd.concat([repo_df, new_repo_df])\n",
    "    repo_df = repo_df.drop_duplicates(subset=['full_name'])\n",
    "    repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "\n",
    "    check_if_older_file_exists(repo_output_path)\n",
    "    repo_df['repo_query_time'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    repo_df.to_csv(repo_output_path, index=False)\n",
    "    # print(\"Repo file updated\", time.time())\n",
    "    repo_df = check_for_entity_in_older_queries(repo_output_path, repo_df, is_large=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_repos = repo_df[repo_df['owner.login'].isin(filtered_users.login)]\n",
    "expanded_core_repos = expanded_core_repos[~expanded_core_repos.full_name.isin(initial_core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12495, 2485)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_core_repos), len(initial_core_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 orgs to existing 270 orgs\n",
      "Added 223 users to existing 1532 users\n",
      "Added 10827 repos to existing 1668 repos\n"
     ]
    }
   ],
   "source": [
    "existing_expanded_core_orgs_file_path = \"../data/derived_files/firstpass_core_orgs.csv\"\n",
    "existing_expanded_core_users_file_path = \"../data/derived_files/firstpass_core_users.csv\"\n",
    "existing_expanded_core_repos_file_path = \"../data/derived_files/firstpass_core_repos.csv\"\n",
    "\n",
    "if os.path.exists(existing_expanded_core_orgs_file_path):\n",
    "    existing_expanded_core_orgs = pd.read_csv(existing_expanded_core_orgs_file_path)\n",
    "    expanded_core_orgs = expanded_core_orgs[~expanded_core_orgs.login.isin(existing_expanded_core_orgs.login)]\n",
    "    print(f\"Added {len(expanded_core_orgs)} orgs to existing {len(existing_expanded_core_orgs)} orgs\")\n",
    "else:\n",
    "    expanded_core_orgs.to_csv(\"../data/derived_files/firstpass_core_orgs.csv\", index=False)\n",
    "\n",
    "if os.path.exists(existing_expanded_core_users_file_path):\n",
    "    existing_expanded_core_users = pd.read_csv(existing_expanded_core_users_file_path)\n",
    "    expanded_core_users = expanded_core_users[~expanded_core_users.login.isin(existing_expanded_core_users.login)]\n",
    "    print(f\"Added {len(expanded_core_users)} users to existing {len(existing_expanded_core_users)} users\")\n",
    "else:\n",
    "    expanded_core_users.to_csv(\"../data/derived_files/firstpass_core_users.csv\", index=False)\n",
    "\n",
    "if os.path.exists(existing_expanded_core_repos_file_path):\n",
    "    existing_expanded_core_repos = pd.read_csv(existing_expanded_core_repos_file_path)\n",
    "    expanded_core_repos = expanded_core_repos[~expanded_core_repos.full_name.isin(existing_expanded_core_repos.full_name)]\n",
    "    print(f\"Added {len(expanded_core_repos)} repos to existing {len(existing_expanded_core_repos)} repos\")\n",
    "else:\n",
    "    expanded_core_repos.to_csv(\"../data/derived_files/firstpass_core_repos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': {213164: 67361568.0},\n",
       " 'node_id': {213164: 'MDEwOlJlcG9zaXRvcnk2NzM2MTU2OA=='},\n",
       " 'name': {213164: 'tankbuster'},\n",
       " 'full_name': {213164: 'thiippal/tankbuster'},\n",
       " 'private': {213164: '0.0'},\n",
       " 'html_url': {213164: 'https://github.com/thiippal/tankbuster'},\n",
       " 'description': {213164: 'An image classifier trained to detect Soviet/Russian military vehicles'},\n",
       " 'fork': {213164: '0.0'},\n",
       " 'url': {213164: 'https://api.github.com/repos/thiippal/tankbuster'},\n",
       " 'forks_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/forks'},\n",
       " 'keys_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/keys{/key_id}'},\n",
       " 'collaborators_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/collaborators{/collaborator}'},\n",
       " 'teams_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/teams'},\n",
       " 'hooks_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/hooks'},\n",
       " 'issue_events_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/issues/events{/number}'},\n",
       " 'events_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/events'},\n",
       " 'assignees_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/assignees{/user}'},\n",
       " 'branches_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/branches{/branch}'},\n",
       " 'tags_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/tags'},\n",
       " 'blobs_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/git/blobs{/sha}'},\n",
       " 'git_tags_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/git/tags{/sha}'},\n",
       " 'git_refs_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/git/refs{/sha}'},\n",
       " 'trees_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/git/trees{/sha}'},\n",
       " 'statuses_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/statuses/{sha}'},\n",
       " 'languages_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/languages'},\n",
       " 'stargazers_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/stargazers'},\n",
       " 'contributors_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/contributors'},\n",
       " 'subscribers_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/subscribers'},\n",
       " 'subscription_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/subscription'},\n",
       " 'commits_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/commits{/sha}'},\n",
       " 'git_commits_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/git/commits{/sha}'},\n",
       " 'comments_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/comments{/number}'},\n",
       " 'issue_comment_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/issues/comments{/number}'},\n",
       " 'contents_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/contents/{+path}'},\n",
       " 'compare_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/compare/{base}...{head}'},\n",
       " 'merges_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/merges'},\n",
       " 'archive_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/{archive_format}{/ref}'},\n",
       " 'downloads_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/downloads'},\n",
       " 'issues_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/issues{/number}'},\n",
       " 'pulls_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/pulls{/number}'},\n",
       " 'milestones_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/milestones{/number}'},\n",
       " 'notifications_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/notifications{?since,all,participating}'},\n",
       " 'labels_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/labels{/name}'},\n",
       " 'releases_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/releases{/id}'},\n",
       " 'deployments_url': {213164: 'https://api.github.com/repos/thiippal/tankbuster/deployments'},\n",
       " 'created_at': {213164: '2016-09-04T18:28:26Z'},\n",
       " 'updated_at': {213164: '2022-09-13T11:01:15Z'},\n",
       " 'pushed_at': {213164: '2017-04-08T14:44:51Z'},\n",
       " 'git_url': {213164: 'git://github.com/thiippal/tankbuster.git'},\n",
       " 'ssh_url': {213164: 'git@github.com:thiippal/tankbuster.git'},\n",
       " 'clone_url': {213164: 'https://github.com/thiippal/tankbuster.git'},\n",
       " 'svn_url': {213164: 'https://github.com/thiippal/tankbuster'},\n",
       " 'homepage': {213164: nan},\n",
       " 'size': {213164: 367798.0},\n",
       " 'stargazers_count': {213164: 28.0},\n",
       " 'watchers_count': {213164: 28.0},\n",
       " 'language': {213164: 'Python'},\n",
       " 'has_issues': {213164: '1.0'},\n",
       " 'has_projects': {213164: '1.0'},\n",
       " 'has_downloads': {213164: '1.0'},\n",
       " 'has_wiki': {213164: '1.0'},\n",
       " 'has_pages': {213164: '0.0'},\n",
       " 'forks_count': {213164: 11.0},\n",
       " 'mirror_url': {213164: nan},\n",
       " 'archived': {213164: '0.0'},\n",
       " 'disabled': {213164: '0.0'},\n",
       " 'open_issues_count': {213164: 1.0},\n",
       " 'license': {213164: nan},\n",
       " 'allow_forking': {213164: '1.0'},\n",
       " 'is_template': {213164: '0.0'},\n",
       " 'web_commit_signoff_required': {213164: '0.0'},\n",
       " 'topics': {213164: '[]'},\n",
       " 'visibility': {213164: 'public'},\n",
       " 'forks': {213164: 11.0},\n",
       " 'open_issues': {213164: 1.0},\n",
       " 'watchers': {213164: 28.0},\n",
       " 'default_branch': {213164: 'master'},\n",
       " 'owner.login': {213164: 'thiippal'},\n",
       " 'owner.id': {213164: 12389814.0},\n",
       " 'owner.node_id': {213164: 'MDQ6VXNlcjEyMzg5ODE0'},\n",
       " 'owner.avatar_url': {213164: 'https://avatars.githubusercontent.com/u/12389814?v=4'},\n",
       " 'owner.gravatar_id': {213164: nan},\n",
       " 'owner.url': {213164: 'https://api.github.com/users/thiippal'},\n",
       " 'owner.html_url': {213164: 'https://github.com/thiippal'},\n",
       " 'owner.followers_url': {213164: 'https://api.github.com/users/thiippal/followers'},\n",
       " 'owner.following_url': {213164: 'https://api.github.com/users/thiippal/following{/other_user}'},\n",
       " 'owner.gists_url': {213164: 'https://api.github.com/users/thiippal/gists{/gist_id}'},\n",
       " 'owner.starred_url': {213164: 'https://api.github.com/users/thiippal/starred{/owner}{/repo}'},\n",
       " 'owner.subscriptions_url': {213164: 'https://api.github.com/users/thiippal/subscriptions'},\n",
       " 'owner.organizations_url': {213164: 'https://api.github.com/users/thiippal/orgs'},\n",
       " 'owner.repos_url': {213164: 'https://api.github.com/users/thiippal/repos'},\n",
       " 'owner.events_url': {213164: 'https://api.github.com/users/thiippal/events{/privacy}'},\n",
       " 'owner.received_events_url': {213164: 'https://api.github.com/users/thiippal/received_events'},\n",
       " 'owner.type': {213164: 'User'},\n",
       " 'owner.site_admin': {213164: 'False'},\n",
       " 'permissions.admin': {213164: '0.0'},\n",
       " 'permissions.maintain': {213164: '0.0'},\n",
       " 'permissions.push': {213164: '0.0'},\n",
       " 'permissions.triage': {213164: '0.0'},\n",
       " 'permissions.pull': {213164: '1.0'},\n",
       " 'license.key': {213164: 'other'},\n",
       " 'license.name': {213164: 'Other'},\n",
       " 'license.spdx_id': {213164: 'NOASSERTION'},\n",
       " 'license.url': {213164: nan},\n",
       " 'license.node_id': {213164: 'MDc6TGljZW5zZTA='},\n",
       " 'repo_query_time': {213164: '2023-06-03'}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_core_repos[0:1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded_core_repos = existing_expanded_core_repos.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Languages of New Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting language: 100%|██████████| 10827/10827 [04:48<00:00, 37.59it/s]\n",
      "Detecting language: 100%|██████████| 223/223 [00:02<00:00, 91.79it/s] \n",
      "Detecting language: 1it [00:00, 1111.07it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc='Detecting language')\n",
    "expanded_core_repos.description = expanded_core_repos.description.fillna('')\n",
    "if 'detected_language' in expanded_core_repos.columns:\n",
    "    needs_language_repos = expanded_core_repos[expanded_core_repos.detected_language.isna()]\n",
    "    existing_language_repos = expanded_core_repos[~expanded_core_repos.detected_language.isna()]\n",
    "else:\n",
    "    needs_language_repos = expanded_core_repos\n",
    "    existing_language_repos = pd.DataFrame() \n",
    "needs_language_repos = needs_language_repos.progress_apply(check_detect_language, axis=1, is_repo=True)\n",
    "expanded_core_repos = pd.concat([existing_language_repos, needs_language_repos])\n",
    "expanded_core_repos = expanded_core_repos.reset_index(drop=True)\n",
    "expanded_core_users.bio = expanded_core_users.bio.fillna('')\n",
    "if 'detected_language' in expanded_core_users.columns:\n",
    "    needs_language_users = expanded_core_users[expanded_core_users.detected_language.isna()]\n",
    "    existing_language_users = expanded_core_users[~expanded_core_users.detected_language.isna()]\n",
    "else:\n",
    "    needs_language_users = expanded_core_users\n",
    "    existing_language_users = pd.DataFrame()\n",
    "needs_language_users = needs_language_users.progress_apply(check_detect_language, axis=1, is_repo=False)\n",
    "expanded_core_users = pd.concat([existing_language_users, needs_language_users])\n",
    "expanded_core_users = expanded_core_users.reset_index(drop=True)\n",
    "expanded_core_orgs.bio = expanded_core_orgs.bio.fillna('')\n",
    "if 'detected_language' in expanded_core_orgs.columns:\n",
    "    needs_language_orgs = expanded_core_orgs[expanded_core_orgs.detected_language.isna()]\n",
    "    existing_language_orgs = expanded_core_orgs[~expanded_core_orgs.detected_language.isna()]\n",
    "else:\n",
    "    needs_language_orgs = expanded_core_orgs\n",
    "    existing_language_orgs = pd.DataFrame()\n",
    "needs_language_orgs = needs_language_orgs.progress_apply(check_detect_language, axis=1, is_repo=False)\n",
    "expanded_core_orgs = pd.concat([existing_language_orgs, needs_language_orgs])\n",
    "expanded_core_orgs = expanded_core_orgs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_users = pd.concat([initial_core_orgs[['login', 'finalized_language']], initial_core_users[['login', 'finalized_language']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_repos = pd.merge(expanded_core_repos, initial_users, left_on='owner.login', right_on='login', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_repos['potential_language'] = np.where(expanded_core_repos['detected_language'].isnull(), expanded_core_repos['finalized_language'], expanded_core_repos['detected_language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_repos = expanded_core_repos.drop(columns=['login', 'finalized_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_core_repos.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(existing_expanded_core_repos_file_path):\n",
    "    expanded_core_repos = pd.concat([expanded_core_repos, existing_expanded_core_repos])\n",
    "    expanded_core_repos.to_csv(\"../data/derived_files/firstpass_core_repos.csv\", index=False)\n",
    "else:\n",
    "    expanded_core_repos.to_csv(\"../data/derived_files/firstpass_core_repos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12495"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_core_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users = pd.merge(expanded_core_users, initial_core_repos[['finalized_language', 'owner.login']], left_on='login', right_on='owner.login', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users = expanded_core_users[~expanded_core_users.duplicated(subset=['login', 'owner.login', 'finalized_language'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users['potential_language'] = np.where(expanded_core_users['detected_language'].isnull(), expanded_core_users['finalized_language'], expanded_core_users['detected_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users = expanded_core_users[~expanded_core_users.duplicated(subset=['login', 'owner.login', 'detected_language', 'finalized_language', 'potential_language'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users = expanded_core_users.drop(columns=['owner.login', 'finalized_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_multiple_languages = expanded_core_users[expanded_core_users.duplicated(subset=['login'])].login.unique().tolist()\n",
    "if len(users_multiple_languages) > 0:\n",
    "    for login in users_multiple_languages:\n",
    "        user_df = expanded_core_users[(expanded_core_users.login == login)]\n",
    "        if len(user_df.potential_language.unique()) > 1:\n",
    "            languages = user_df.potential_language.unique().tolist()\n",
    "            languages = [x for x in languages if str(x) != 'nan']\n",
    "            updated_languages = [x for x in languages if ',' in x]\n",
    "            if len(updated_languages) == 0:\n",
    "                updated_languages = ', '.join(languages)\n",
    "            else:\n",
    "                updated_languages = updated_languages[0]\n",
    "            expanded_core_users.loc[expanded_core_users.login == login, 'potential_language'] = updated_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_users = expanded_core_users.drop_duplicates(subset=['login', 'potential_language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(existing_expanded_core_users_file_path):\n",
    "    expanded_core_users = pd.concat([expanded_core_users, existing_expanded_core_users])\n",
    "expanded_core_users.to_csv(\"../data/derived_files/firstpass_core_users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_core_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_core_orgs = pd.merge(expanded_core_orgs, initial_core_repos[['finalized_language', 'owner.login']], left_on='login', right_on='owner.login', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_core_orgs = expanded_core_orgs[~expanded_core_orgs.duplicated(subset=['login', 'owner.login', 'finalized_language'])]\n",
    "expanded_core_orgs['potential_language'] = np.where(expanded_core_orgs['detected_language'].isnull(), expanded_core_orgs['finalized_language'], expanded_core_orgs['detected_language'])\n",
    "expanded_core_orgs = expanded_core_orgs[~expanded_core_orgs.duplicated(subset=['login', 'owner.login', 'detected_language', 'finalized_language', 'potential_language'])]\n",
    "expanded_core_orgs = expanded_core_orgs.drop(columns=['owner.login', 'finalized_language'])\n",
    "len(expanded_core_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs_multiple_languages = expanded_core_orgs[expanded_core_orgs.duplicated(subset=['login'])].login.unique().tolist()\n",
    "if len(orgs_multiple_languages) > 0:\n",
    "    for login in orgs_multiple_languages:\n",
    "        user_df = expanded_core_orgs[(expanded_core_orgs.login == login)]\n",
    "        if len(user_df.potential_language.unique()) > 1:\n",
    "            languages = user_df.potential_language.unique().tolist()\n",
    "            languages = [x for x in languages if str(x) != 'nan']\n",
    "            updated_languages = [x for x in languages if ',' in x]\n",
    "            if len(updated_languages) == 0:\n",
    "                updated_languages = ', '.join(languages)\n",
    "            else:\n",
    "                updated_languages = updated_languages[0]\n",
    "            expanded_core_orgs.loc[expanded_core_orgs.login == login, 'potential_language'] = updated_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_core_orgs = expanded_core_orgs.drop_duplicates(subset=['login', 'potential_language'])\n",
    "len(expanded_core_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(existing_expanded_core_orgs_file_path):\n",
    "    expanded_core_orgs = pd.concat([expanded_core_orgs, existing_expanded_core_orgs])\n",
    "expanded_core_orgs.to_csv(\"../data/derived_files/firstpass_core_orgs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "values_and_versions_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
