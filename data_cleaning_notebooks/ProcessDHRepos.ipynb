{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DH Repos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be run multiple times as we add new repositories to the DH repo list. It will take the data from the DH repo list and process it through making additional calls to GitHub's APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Set Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_return_error_file, read_combine_files, check_total_pages, check_for_joins_in_older_queries\n",
    "from data_generation_scripts.generate_search_data import get_initial_search_datasets\n",
    "from data_generation_scripts.generate_repo_metadata import get_repo_languages, get_repo_labels, get_repo_tags,  get_repo_profile, get_total_commits\n",
    "from data_generation_scripts.generate_repo_users_interactions import get_repos_user_actors\n",
    "from data_generation_scripts.generate_repo_metadata import check_total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = check_rate_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12495"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_repos_path = \"../data/derived_files/firstpass_core_repos.csv\"\n",
    "core_repos = pd.read_csv(core_repos_path)\n",
    "len(core_repos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Missing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields = pd.read_csv('../data/metadata_files/repo_url_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields.loc[counts_fields.url_type == 'review_comments_url', 'count_type'] = 'review_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(repo_df, url_type, count_type, overwrite_existing_temp_files = False):\n",
    "    if count_type in repo_df.columns:\n",
    "        needs_counts = repo_df[repo_df[count_type].isna()]\n",
    "        has_counts = repo_df[repo_df[count_type].notna()]\n",
    "    else:\n",
    "        needs_counts = repo_df\n",
    "        has_counts = pd.DataFrame()\n",
    "        \n",
    "    if len(has_counts) == len(repo_df):\n",
    "        repo_df = has_counts\n",
    "    else:\n",
    "        needs_counts = check_total_results(needs_counts, url_type, overwrite_existing_temp_files)\n",
    "        repo_df = pd.concat([needs_counts, has_counts])\n",
    "    return repo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pulls_count for pulls_url\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "skip_types = ['review_comments_url', 'commits_url', 'collaborators_url']\n",
    "overwrite_existing_temp_files = True\n",
    "for index, row in counts_fields.iterrows():\n",
    "    if (row.url_type not in skip_types):\n",
    "        count_type = row.url_type.split(\"_\")[0] + \"_count\"\n",
    "        print(f\"Getting {count_type} for {row['url_type']}\")\n",
    "        if (count_type not in core_repos.columns) or (core_repos[count_type].isna().any()):\n",
    "            core_repos = get_counts(core_repos, row['url_type'], count_type, overwrite_existing_temp_files)\n",
    "        row['count_type'] = count_type\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_repos.to_csv(core_repos_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_fields.to_csv('../data/metadata_files/repo_url_cols.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'contributors_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "contributors_df, users_df = get_repos_user_actors(core_repos, '../data/join_files/repo_contributors_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "contributors_errors_df = check_return_error_file('../data/error_logs/repo_contributors_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors_df = contributors_df[contributors_df['repo_full_name'].isin(core_repos['full_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2264 repos, we found 5210 contributors, of which 3656 are unique. There were 6 errors in getting contributors (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(contributors_df)} contributors, of which {len(contributors_df.login.unique())} are unique. There were {len(contributors_errors_df)} errors in getting contributors (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Starrers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'stargazers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login']\n",
    "stargazers_df, users_df = get_repos_user_actors(core_repos, '../data/join_files/repo_stargazers_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "stargazers_errors_df = check_return_error_file('../data/error_logs/repo_stargazers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazers_df = stargazers_df[stargazers_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2264 repos, we found 35225 stars, which were created by 9182 unique users on 834 unique repos. There were 2 errors in getting stargazers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(stargazers_df)} stars, which were created by {len(stargazers_df['user.login'].unique())} unique users on {stargazers_df.repo_full_name.nunique()} unique repos. There were {len(stargazers_errors_df)} errors in getting stargazers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'subscribers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "subscribers_df, users_df = get_repos_user_actors(core_repos, '../data/join_files/repo_subscribers_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "subscribers_errors_df = check_return_error_file('../data/error_logs/repo_subscribers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribers_df = subscribers_df[subscribers_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2264 repos, we found 9969 subscribers, of which 3271 are unique. There were 11 errors in getting subscribers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subscribers_df)} subscribers, of which {len(subscribers_df.login.unique())} are unique. There were {len(subscribers_errors_df)} errors in getting subscribers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannot get repo collaborators\n",
    "\n",
    "Need Push Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_url_field = 'collaborators_url'\n",
    "# load_existing_data = False\n",
    "# is_stargazers = False\n",
    "# collaborators_df, users_df = get_repos_user_actors(repo_df, '../data/repo_collaborators_join_dataset.csv', '../data/users_dataset.csv', rates_df, get_url_field, load_existing_data, is_stargazers)\n",
    "# collaborators_errors_df = check_return_error_file('../data/error_logs/repo_collaborators_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Forks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'forks_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'full_name']\n",
    "forks_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_forks_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "forks_errors_df = check_return_error_file('../data/error_logs/repo_forks_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "forks_df = forks_df[forks_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2264 repos, we found 6379 forks, of which 4154 are unique. There were 1 errors in getting forks (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(forks_df)} forks, of which {len(forks_df['owner.login'].unique())} are unique. There were {len(forks_errors_df)} errors in getting forks (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'issues_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "issues_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_issues_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "issues_errors_df = check_return_error_file('../data/error_logs/repo_issues_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_df = issues_df[issues_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2264 repos, we found 33256 issues, which come from 622 unique repos and were created by 1476 unique users. There were 1 errors in getting issues (likely repos that have no issues longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(issues_df)} issues, which come from {len(issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_df['user.login'])])} unique users. There were {len(issues_errors_df)} errors in getting issues (likely repos that have no issues longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Issue Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "issues_comments_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/issues_comments_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "issues_comments_errors_df = check_return_error_file('../data/error_logs/issues_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_comments_df = issues_comments_df[issues_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 33256 repos with issues, we found 50678 comments, which come from 622 unique repos and were created by 1184 unique users. There were 2477 errors in getting issues comments (likely issues comments longer that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(issues_df)} repos with issues, we found {len(issues_comments_df)} comments, which come from {len(issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_comments_df['user.login'])])} unique users. There were {len(issues_comments_errors_df)} errors in getting issues comments (likely issues comments longer that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'pulls_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['id', 'repo_full_name', 'user.login', 'head.user.login']\n",
    "join_unique_field = 'repo_full_name'\n",
    "pulls_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_pulls_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "pulls_errors_df = check_return_error_file('../data/error_logs/repo_pulls_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_df = pulls_df[pulls_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2485 repos, we found 15752 pulls, which come from 478 unique repos and were created by 927 unique users. There were 0 errors in getting pulls (likely repos that have no pulls longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(pulls_df)} pulls, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_df['user.login'])])} unique users. There were {len(pulls_errors_df)} errors in getting pulls (likely repos that have no pulls longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Pull Request Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'review_count' not in pulls_df.columns:\n",
    "    url_type = \"review_comments_url\"\n",
    "    count_type = \"review_count\"\n",
    "    pulls_df = get_counts(pulls_df, url_type, count_type, overwrite_existing_temp_files=False)\n",
    "    pulls_df.to_csv('../data/large_files/join_files/repo_pulls_join_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulls_df.review_count.sum() / 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'review_comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['repo_full_name', 'user.login', 'url']\n",
    "join_unique_field = 'repo_full_name'\n",
    "pulls_comments_df, users_df = get_repos_user_actors(pulls_df, '../data/large_files/join_files/pulls_comments_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "pulls_comments_errors_df = check_return_error_file('../data/error_logs/pulls_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_comments_df = pulls_comments_df[pulls_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 15752 repos with pulls, we found 6140 comments, which come from 478 unique repos and were created by 164 unique users. There were 16 errors in getting pulls comments (likely repos that have no pulls comments longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(pulls_df)} repos with pulls, we found {len(pulls_comments_df)} comments, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_comments_df['user.login'])])} unique users. There were {len(pulls_comments_errors_df)} errors in getting pulls comments (likely repos that have no pulls comments longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_total_commits(core_repos, '../data/large_files/entity_files/subset_repos_dataset_with_commits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2108 repos, we found 241355 total commits, which considering the 5000 rate limit will take 48.271 hours to get.\n"
     ]
    }
   ],
   "source": [
    "core_repos['cleaned_total_commits'] = core_repos.total_commits.astype(int)\n",
    "print(f\"From {len(core_repos)} repos, we found {core_repos.cleaned_total_commits.sum()} total commits, which considering the 5000 rate limit will take {core_repos.cleaned_total_commits.sum()/5000} hours to get.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'commits_url'\n",
    "load_existing_files = False\n",
    "load_existing_temp_files = True\n",
    "commits_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_commits_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, load_existing_temp_files)\n",
    "commits_errors_df = check_return_error_file('../data/error_logs/repo_commits_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Explore Repo Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/large_files/join_files/pulls_comments_join_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/comments/95563117'},\n",
       " 'pull_request_review_id': {0: 16120671.0},\n",
       " 'id': {0: 95563117},\n",
       " 'node_id': {0: 'MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk1NTYzMTE3'},\n",
       " 'diff_hunk': {0: \"@@ -432,6 +432,30 @@ Example of returned tag data:\\n \\n         ['Authority in literature', 'Errata']\\n \\n+===================\\n+Retrieving Version Information\\n+===================\\n+\\n+The Zotero API recommends requesting version information for all (or all changed) items and collections when implementing syncing.  The following convience methods simplify this process.  Note that by default these methods impose no limit on the number of items/collection versions returned.  To retrieve only the versions of items changed after a given version use the since search/request parameter.\"},\n",
       " 'path': {0: 'doc/index.rst'},\n",
       " 'position': {0: nan},\n",
       " 'original_position': {0: 56},\n",
       " 'commit_id': {0: '5c03e0922132f0e41b32f156c465c16d5f7aadb9'},\n",
       " 'original_commit_id': {0: '8241fac6514d10790cdaad5e6237160d291fa4a6'},\n",
       " 'body': {0: 'Can we clarify what the since / search parameter is? I think it should be rendered `as code`, and include a link to https://www.zotero.org/support/dev/web_api/v3/syncing'},\n",
       " 'created_at': {0: '2017-01-11T11:47:11Z'},\n",
       " 'updated_at': {0: '2017-01-11T12:15:19Z'},\n",
       " 'html_url': {0: 'https://github.com/urschrei/pyzotero/pull/64#discussion_r95563117'},\n",
       " 'pull_request_url': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/64'},\n",
       " 'author_association': {0: 'OWNER'},\n",
       " 'start_line': {0: nan},\n",
       " 'original_start_line': {0: nan},\n",
       " 'start_side': {0: nan},\n",
       " 'line': {0: nan},\n",
       " 'original_line': {0: 439},\n",
       " 'side': {0: 'RIGHT'},\n",
       " 'user.login': {0: 'urschrei'},\n",
       " 'user.id': {0: 131862},\n",
       " 'user.node_id': {0: 'MDQ6VXNlcjEzMTg2Mg=='},\n",
       " 'user.avatar_url': {0: 'https://avatars.githubusercontent.com/u/131862?v=4'},\n",
       " 'user.gravatar_id': {0: nan},\n",
       " 'user.url': {0: 'https://api.github.com/users/urschrei'},\n",
       " 'user.html_url': {0: 'https://github.com/urschrei'},\n",
       " 'user.followers_url': {0: 'https://api.github.com/users/urschrei/followers'},\n",
       " 'user.following_url': {0: 'https://api.github.com/users/urschrei/following{/other_user}'},\n",
       " 'user.gists_url': {0: 'https://api.github.com/users/urschrei/gists{/gist_id}'},\n",
       " 'user.starred_url': {0: 'https://api.github.com/users/urschrei/starred{/owner}{/repo}'},\n",
       " 'user.subscriptions_url': {0: 'https://api.github.com/users/urschrei/subscriptions'},\n",
       " 'user.organizations_url': {0: 'https://api.github.com/users/urschrei/orgs'},\n",
       " 'user.repos_url': {0: 'https://api.github.com/users/urschrei/repos'},\n",
       " 'user.events_url': {0: 'https://api.github.com/users/urschrei/events{/privacy}'},\n",
       " 'user.received_events_url': {0: 'https://api.github.com/users/urschrei/received_events'},\n",
       " 'user.type': {0: 'User'},\n",
       " 'user.site_admin': {0: False},\n",
       " '_links.self.href': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/comments/95563117'},\n",
       " '_links.html.href': {0: 'https://github.com/urschrei/pyzotero/pull/64#discussion_r95563117'},\n",
       " '_links.pull_request.href': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/64'},\n",
       " 'reactions.url': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/comments/95563117/reactions'},\n",
       " 'reactions.total_count': {0: 0},\n",
       " 'reactions.+1': {0: 0},\n",
       " 'reactions.-1': {0: 0},\n",
       " 'reactions.laugh': {0: 0},\n",
       " 'reactions.hooray': {0: 0},\n",
       " 'reactions.confused': {0: 0},\n",
       " 'reactions.heart': {0: 0},\n",
       " 'reactions.rocket': {0: 0},\n",
       " 'reactions.eyes': {0: 0},\n",
       " 'in_reply_to_id': {0: nan},\n",
       " 'repo_id': {0: 101038100},\n",
       " 'repo_url': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/64'},\n",
       " 'repo_html_url': {0: 'https://github.com/urschrei/pyzotero/pull/64'},\n",
       " 'repo_full_name': {0: 'Jean-Baptiste-Camps/OCR_workshop'},\n",
       " 'review_comments_url': {0: 'https://api.github.com/repos/urschrei/pyzotero/pulls/64/comments'},\n",
       " 'repo_query_time': {0: '2022-12-26'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_joins_in_older_queries(\"../data/large_files/join_files/pulls_comments_join_dataset.csv\", test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "error_file_path = \"../data/error_logs/repo_profile_errors.csv\"\n",
    "temp_repo_dir = \"../data/temp/repo_profile/\"\n",
    "core_repos = get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_languages(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_labels(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_tags(core_repos, repo_output_path, rates_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('values_and_versions_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92b14f494f00d59c3bde62efd18715990e48408449b0a7aa8b9d827d8bcdd450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
