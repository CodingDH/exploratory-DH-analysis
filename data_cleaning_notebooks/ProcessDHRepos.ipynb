{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process DH Repos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be run multiple times as we add new repositories to the DH repo list. It will take the data from the DH repo list and process it through making additional calls to GitHub's APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Set Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from data_generation_scripts.utils import check_rate_limit, check_return_error_file, read_combine_files, check_total_pages, check_for_joins_in_older_queries, check_add_users, combined_updated_users, check_for_entity_in_older_queries\n",
    "from data_generation_scripts.generate_search_data import get_initial_search_datasets\n",
    "from data_generation_scripts.generate_repo_metadata import get_repo_languages, get_repo_labels, get_repo_tags,  get_repo_profile, get_total_commits\n",
    "from data_generation_scripts.generate_repo_users_interactions import get_repos_user_actors\n",
    "from data_generation_scripts.generate_repo_metadata import check_total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_df = check_rate_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_core_repos = pd.read_csv(\"../data/derived_files/initial_core_repos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12495"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_repos_path = \"../data/derived_files/firstpass_core_repos.csv\"\n",
    "core_repos = pd.read_csv(core_repos_path)\n",
    "len(core_repos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Missing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields = pd.read_csv('../data/metadata_files/repo_url_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fields.loc[counts_fields.url_type == 'review_comments_url', 'count_type'] = 'review_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(repo_df, url_type, count_type, overwrite_existing_temp_files = False):\n",
    "    if count_type in repo_df.columns:\n",
    "        needs_counts = repo_df[repo_df[count_type].isna()]\n",
    "        has_counts = repo_df[repo_df[count_type].notna()]\n",
    "    else:\n",
    "        needs_counts = repo_df\n",
    "        has_counts = pd.DataFrame()\n",
    "        \n",
    "    if len(has_counts) == len(repo_df):\n",
    "        repo_df = has_counts\n",
    "    else:\n",
    "        needs_counts = check_total_results(needs_counts, url_type, overwrite_existing_temp_files)\n",
    "        repo_df = pd.concat([needs_counts, has_counts])\n",
    "    return repo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pulls_count for pulls_url\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "skip_types = ['review_comments_url', 'commits_url', 'collaborators_url']\n",
    "overwrite_existing_temp_files = True\n",
    "for index, row in counts_fields.iterrows():\n",
    "    if (row.url_type not in skip_types):\n",
    "        count_type = row.url_type.split(\"_\")[0] + \"_count\"\n",
    "        print(f\"Getting {count_type} for {row['url_type']}\")\n",
    "        if (count_type not in core_repos.columns) or (core_repos[count_type].isna().any()):\n",
    "            core_repos = get_counts(core_repos, row['url_type'], count_type, overwrite_existing_temp_files)\n",
    "        row['count_type'] = count_type\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_repos.to_csv(core_repos_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_fields.to_csv('../data/metadata_files/repo_url_cols.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'contributors_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "contributors_df, users_df = get_repos_user_actors(core_repos, '../data/join_files/repo_contributors_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "contributors_errors_df = check_return_error_file('../data/error_logs/repo_contributors_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_contributors_df = contributors_df[(contributors_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120151, 114763)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contributors_df), len(subset_contributors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 114763 contributors, of which 64856 are unique. There were 6 errors in getting contributors (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_contributors_df)} contributors, of which {len(subset_contributors_df.login.unique())} are unique. There were {len(contributors_errors_df)} errors in getting contributors (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Starrers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'stargazers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login']\n",
    "stargazers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_stargazers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "stargazers_errors_df = check_return_error_file('../data/error_logs/repo_stargazers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_stargazers_df = stargazers_df[(stargazers_df['repo_full_name'].isin(core_repos['full_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33097, 11263)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stargazers_df), len(subset_stargazers_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'subscribers_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'login']\n",
    "subscribers_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_subscribers_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "subscribers_errors_df = check_return_error_file('../data/error_logs/repo_subscribers_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_subscribers_df = subscribers_df[subscribers_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47047, 25149)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subscribers_df), len(subset_subscribers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 25149 subset_subscribers, of which 2747 are unique. There were 11 errors in getting subscribers (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_subscribers_df)} subset_subscribers, of which {len(subset_subscribers_df.login.unique())} are unique. There were {len(subscribers_errors_df)} errors in getting subscribers (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cannot get repo collaborators\n",
    "\n",
    "Need Push Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_url_field = 'collaborators_url'\n",
    "# load_existing_data = False\n",
    "# is_stargazers = False\n",
    "# collaborators_df, users_df = get_repos_user_actors(repo_df, '../data/repo_collaborators_join_dataset.csv', '../data/users_dataset.csv', rates_df, get_url_field, load_existing_data, is_stargazers)\n",
    "# collaborators_errors_df = check_return_error_file('../data/error_logs/repo_collaborators_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Forks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'forks_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'full_name']\n",
    "forks_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_forks_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "forks_errors_df = check_return_error_file('../data/error_logs/repo_forks_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_forks_df = forks_df[forks_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32329, 5594)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forks_df), len(subset_forks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 5594 subset_forks, of which 4294 are unique. There were 1 errors in getting forks (likely user accounts that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_forks_df)} subset_forks, of which {len(subset_forks_df['owner.login'].unique())} are unique. There were {len(forks_errors_df)} errors in getting forks (likely user accounts that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'issues_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "issues_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_issues_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "issues_errors_df = check_return_error_file('../data/error_logs/repo_issues_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_issues_df = issues_df[issues_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 12495 repos, we found 41626 issues, which come from 1336 unique repos and were created by 3062 unique users. There were 1 errors in getting issues (likely repos that have no issues longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(subset_issues_df)} issues, which come from {len(subset_issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_df['user.login'])])} unique users. There were {len(issues_errors_df)} errors in getting issues (likely repos that have no issues longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Issue Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "join_unique_field = 'repo_full_name'\n",
    "filter_fields = ['repo_full_name', 'user.login', 'id']\n",
    "issues_comments_df, users_df = get_repos_user_actors(issues_df, '../data/large_files/join_files/issues_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "issues_comments_errors_df = check_return_error_file('../data/error_logs/issues_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_comments_df = issues_comments_df[issues_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 90552 repos with issues, we found 36515 comments, which come from 1999 unique repos and were created by 1184 unique users. There were 2505 errors in getting issues comments (likely issues comments longer that no longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(issues_df)} repos with issues, we found {len(issues_comments_df)} comments, which come from {len(issues_df.repository_url.unique())} unique repos and were created by {len(users_df[users_df.login.isin(issues_comments_df['user.login'])])} unique users. There were {len(issues_comments_errors_df)} errors in getting issues comments (likely issues comments longer that no longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'pulls_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['id', 'repo_full_name', 'user.login', 'head.user.login']\n",
    "join_unique_field = 'repo_full_name'\n",
    "pulls_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_pulls_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "pulls_errors_df = check_return_error_file('../data/error_logs/repo_pulls_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulls_df.review_count.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_df = pulls_df[pulls_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2485 repos, we found 15752 pulls, which come from 478 unique repos and were created by 927 unique users. There were 0 errors in getting pulls (likely repos that have no pulls longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(core_repos)} repos, we found {len(pulls_df)} pulls, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_df['user.login'])])} unique users. There were {len(pulls_errors_df)} errors in getting pulls (likely repos that have no pulls longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Repo Pull Request Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_type = \"review_comments_url\"\n",
    "count_type = \"review_count\"\n",
    "pulls_df = get_counts(pulls_df, url_type, count_type, overwrite_existing_temp_files=False)\n",
    "pulls_df.to_csv('../data/large_files/join_files/repo_pulls_join_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'review_comments_url'\n",
    "load_existing_files = True\n",
    "overwrite_existing_temp_files = False\n",
    "filter_fields = ['repo_full_name', 'user.login', 'url']\n",
    "join_unique_field = 'repo_full_name'\n",
    "pulls_comments_df, users_df = get_repos_user_actors(pulls_df, '../data/large_files/join_files/pulls_comments_join_dataset.csv', '../data/large_files/entity_files/users_dataset.csv', get_url_field, load_existing_files, overwrite_existing_temp_files, join_unique_field, filter_fields)\n",
    "pulls_comments_errors_df = check_return_error_file('../data/error_logs/pulls_comments_join_dataset_errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_comments_df = pulls_comments_df[pulls_comments_df.repo_full_name.isin(core_repos.full_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 15752 repos with pulls, we found 6140 comments, which come from 478 unique repos and were created by 164 unique users. There were 16 errors in getting pulls comments (likely repos that have no pulls comments longer exist).\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {len(pulls_df)} repos with pulls, we found {len(pulls_comments_df)} comments, which come from {len(pulls_df.repo_id.unique())} unique repos and were created by {len(users_df[users_df.login.isin(pulls_comments_df['user.login'])])} unique users. There were {len(pulls_comments_errors_df)} errors in getting pulls comments (likely repos that have no pulls comments longer exist).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Repo Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_total_commits(core_repos, '../data/large_files/entity_files/subset_repos_dataset_with_commits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2108 repos, we found 241355 total commits, which considering the 5000 rate limit will take 48.271 hours to get.\n"
     ]
    }
   ],
   "source": [
    "core_repos['cleaned_total_commits'] = core_repos.total_commits.astype(int)\n",
    "print(f\"From {len(core_repos)} repos, we found {core_repos.cleaned_total_commits.sum()} total commits, which considering the 5000 rate limit will take {core_repos.cleaned_total_commits.sum()/5000} hours to get.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_url_field = 'commits_url'\n",
    "load_existing_files = False\n",
    "load_existing_temp_files = True\n",
    "commits_df, users_df = get_repos_user_actors(core_repos, '../data/large_files/join_files/repo_commits_join_dataset.csv', '../data/entity_files/users_dataset.csv', get_url_field, load_existing_files, load_existing_temp_files)\n",
    "commits_errors_df = check_return_error_file('../data/error_logs/repo_commits_join_dataset_errors.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Explore Repo Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_output_path = \"../data/large_files/entity_files/repos_dataset.csv\"\n",
    "error_file_path = \"../data/error_logs/repo_profile_errors.csv\"\n",
    "temp_repo_dir = \"../data/temp/repo_profile/\"\n",
    "core_repos = get_repo_profile(core_repos, repo_output_path, rates_df, error_file_path, temp_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_languages(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_labels(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_repos = get_repo_tags(core_repos, repo_output_path, rates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_actors_output_path = '../data/large_files/join_files/pulls_comments_join_dataset.csv'\n",
    "test = pd.read_csv(f\"../data/error_logs/{repo_actors_output_path.split('/')[-1].split('.csv')[0]}_errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_full_name</th>\n",
       "      <th>error_time</th>\n",
       "      <th>error_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180179382</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>ScandinavianSection-UCLA/Nexus2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>444790707</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360156974</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355247192</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225865362</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>162486264</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>148198666</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>142989558</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>140403014</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119517224</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>114183363</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>CDRH/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1273341415</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>eeditiones/tei-publisher-app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1268035772</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>eeditiones/tei-publisher-app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1186399168</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>eeditiones/tei-publisher-app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>951801787</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>eeditiones/tei-publisher-app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>604345311</td>\n",
       "      <td>1.685518e+09</td>\n",
       "      <td>eeditiones/tei-publisher-app</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    repo_full_name    error_time                        error_url\n",
       "0        180179382  1.685518e+09  ScandinavianSection-UCLA/Nexus2\n",
       "1        444790707  1.685518e+09                         CDRH/api\n",
       "2        360156974  1.685518e+09                         CDRH/api\n",
       "3        355247192  1.685518e+09                         CDRH/api\n",
       "4        225865362  1.685518e+09                         CDRH/api\n",
       "5        162486264  1.685518e+09                         CDRH/api\n",
       "6        148198666  1.685518e+09                         CDRH/api\n",
       "7        142989558  1.685518e+09                         CDRH/api\n",
       "8        140403014  1.685518e+09                         CDRH/api\n",
       "9        119517224  1.685518e+09                         CDRH/api\n",
       "10       114183363  1.685518e+09                         CDRH/api\n",
       "11      1273341415  1.685518e+09     eeditiones/tei-publisher-app\n",
       "12      1268035772  1.685518e+09     eeditiones/tei-publisher-app\n",
       "13      1186399168  1.685518e+09     eeditiones/tei-publisher-app\n",
       "14       951801787  1.685518e+09     eeditiones/tei-publisher-app\n",
       "15       604345311  1.685518e+09     eeditiones/tei-publisher-app"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('values_and_versions_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92b14f494f00d59c3bde62efd18715990e48408449b0a7aa8b9d827d8bcdd450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
