# Paper Structure
 
1. How do you even identify a distinct identity on Github? Limitations of the platform for data mining. Problem of intended use cases - platform is for projects and teams, not really a social network site in its structure.
    - Identification is hard.
        - Existing sources on datamining Github
    - Errors (delete traces)

2. What did we find with the data that we could get.

3. Github as an **archive** / source -> strength and limitations.

Push deadline until middle of December? Jeri will email.


Really focus on making the dataset (EDA and data curation)

----

Things to explore
- Who is on multiple projects (how many and which)
- 

What can we get vs can't get (for the paper)
- what is not available through the API -> 
    - wikis, 
    - projects, 
    - commits vs contributions
    - PRs vs Issues
    - Teams (need organizational access)
    - private repos (not even number)

- descriptions are from the time of query

- dig into follower / following dynamics.

- Can we combine with other datasets - index of DH abstracts / DH twitter


--

note -> side piece on the hidden labor of Github - how to do a ton of work that does not get captured.
- maintanence work that does not get captured.

---
What counts as a "core" user for exploring out?
- have described themselves as "digital humanities"
- own a repo that is described as "digital humanities"
- have contributed to at least two repos that have been described as "digital humanities"
 
- We do want the repos that our core users own. 



# Tasks
[ ] rerun user followers, following, starrers with core_users defined (Zoe will take care of this)
[ ] get repos owned by core users (use generate_user_repo_interactions.py, run from commandline. save the repos to new join (user_repos_join data set. Unique repos go into the entity file (repo))) (Jeri will take care of this)
[ ] figure out events API to get everything?? for core users (save for next week)
    - might end up changing what counts as core_repos
[ ] Email about extension (jeri will do this)


```python
user_df = pd.read_csv("../data/entity_files/users_dataset.csv")
repo_df = pd.read_csv("../data/large_files/entity_files/repos_dataset.csv", low_memory=False)
search_queries_repo_join_df = pd.read_csv("../data/join_files/search_queries_repo_join_dataset.csv")
search_queries_user_join_df = pd.read_csv("../data/join_files/search_queries_user_join_dataset.csv")
contributors_df = pd.read_csv('../data/join_files/repo_contributors_join_dataset.csv')
contributors_counts = contributors_df.groupby(['login']).size().reset_index(name='counts')
top_contributors = contributors_counts[contributors_counts.counts > 1]
core_repos = repo_df[repo_df["id"].isin(search_queries_repo_join_df["id"].unique())]
core_users = user_df[(user_df.login.isin(top_contributors.login)) | (user_df.login.isin(search_queries_user_join_df.login)) | (user_df.login.isin(core_repos['owner.login']))].drop_duplicates(subset=['login'])
```